{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Voice-Powered RAG Agent with NVIDIA Nemotron Models\n",
    "\n",
    "This notebook walks you through building an end-to-end AI agent that combines voice input, multimodal retrieval, safety guardrails, and long-context reasoning using NVIDIA's Nemotron model family.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        VOICE-POWERED RAG AGENT                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚   â”‚  Voice  â”‚â”€â”€â”€>â”‚   ASR   â”‚â”€â”€â”€>â”‚   RAG   â”‚â”€â”€â”€>â”‚   LLM   â”‚â”€â”€â”€>â”‚ Safety  â”‚   â”‚\n",
    "â”‚   â”‚  Input  â”‚    â”‚ (NeMo)  â”‚    â”‚ Embed+  â”‚    â”‚ Reason  â”‚    â”‚  Guard  â”‚   â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Rerank  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚       â”‚\n",
    "â”‚                                      â”‚                              â”‚       â”‚\n",
    "â”‚                                      v                              v       â”‚\n",
    "â”‚                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚                                 â”‚  FAISS  â”‚                   â”‚  Safe   â”‚   â”‚\n",
    "â”‚                                 â”‚  Index  â”‚                   â”‚ Responseâ”‚   â”‚\n",
    "â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Models Used\n",
    "\n",
    "| Component | Model | Deployment |\n",
    "|-----------|-------|------------|\n",
    "| Speech-to-Text | `nemotron-speech-streaming-en-0.6b` | Self-hosted (NeMo) |\n",
    "| Embeddings | `llama-nemotron-embed-vl-1b-v2` | Self-hosted (Transformers) |\n",
    "| Reranking | `llama-nemotron-rerank-vl-1b-v2` | Self-hosted (Transformers) |\n",
    "| Vision-Language | `nemotron-nano-12b-v2-vl` | NVIDIA API |\n",
    "| Reasoning | `nemotron-3-nano-30b-a3b` | NVIDIA API |\n",
    "| Safety | `Llama-3.1-Nemotron-Safety-Guard-8B-v3` | Self-hosted (Transformers) |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NVIDIA GPU with 24GB+ VRAM (for self-hosted models)\n",
    "- NVIDIA API key (for cloud-hosted reasoning models)\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Environment Setup\n",
    "\n",
    "Before we begin, we need to install the required dependencies and configure API access.\n",
    "\n",
    "**What gets installed:**\n",
    "- **LangChain v1.0**: Modern agent orchestration with `create_agent` API\n",
    "- **langchain-nvidia-ai-endpoints**: ChatNVIDIA integration for NVIDIA API\n",
    "- **Transformers + PyTorch**: For running local embedding, reranking, and safety models\n",
    "- **FAISS**: Vector similarity search\n",
    "- **NeMo Toolkit**: NVIDIA's ASR framework\n",
    "- **ipywebrtc**: Audio recording widget for Jupyter\n",
    "\n",
    "```bash\n",
    "# Install core dependencies (LangChain v1.0+)\n",
    "pip install langchain langchain-nvidia-ai-endpoints faiss-cpu transformers torch pillow requests jinja2 soundfile\n",
    "\n",
    "# Install NeMo for ASR\n",
    "pip install nemo_toolkit[asr]\n",
    "\n",
    "# Install audio recording widget\n",
    "pip install ipywebrtc\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set up NVIDIA API key for cloud-hosted models\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API Key: \")\n",
    "\n",
    "NVIDIA_API_KEY = os.environ[\"NVIDIA_API_KEY\"]\n",
    "\n",
    "print(\"âœ… Environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU available: NVIDIA RTX 6000 Ada Generation\n",
      "   Memory: 47.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Self-hosted models will run slowly on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Ground the Agent with Multimodal RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) grounds our agent in real data, preventing hallucinations by providing factual context for every response.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MULTIMODAL RAG PIPELINE                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   INDEXING (Offline)                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚   â”‚  Text    â”‚â”€â”€â”€>â”‚  Embed   â”‚â”€â”€â”€>â”‚  FAISS   â”‚                  â”‚\n",
    "â”‚   â”‚  Docs    â”‚    â”‚  Model   â”‚    â”‚  Index   â”‚                  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                                        â”‚\n",
    "â”‚   â”‚  Images  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   RETRIEVAL (Online)                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Query   â”‚â”€â”€â”€>â”‚  Embed   â”‚â”€â”€â”€>â”‚  Search  â”‚â”€â”€â”€>â”‚  Rerank  â”‚  â”‚\n",
    "â”‚   â”‚          â”‚    â”‚  Query   â”‚    â”‚  Top-K   â”‚    â”‚  Top-N   â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 2.1 Load the Embedding Model\n",
    "\n",
    "The `llama-nemotron-embed-vl-1b-v2` model creates semantic vector representations of both text and images. This allows us to:\n",
    "\n",
    "- **Text-only embedding**: Standard document search\n",
    "- **Image-only embedding**: Search over screenshots, diagrams, slides\n",
    "- **Image+Text pairs**: Maximum retrieval accuracy for rich documents\n",
    "\n",
    "The model uses different context lengths for each mode to optimize quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Logged into HuggingFace!\n",
      "Loading embedding model: nvidia/llama-nemotron-embed-vl-1b-v2...\n",
      "âœ… Embedding model loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "from typing import List, Optional\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Authenticate with HuggingFace for private/gated models\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HuggingFace Token: \")\n",
    "\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "print(\"âœ… Logged into HuggingFace!\")\n",
    "\n",
    "class NemotronVLEmbeddings:\n",
    "    \"\"\"Custom multimodal embedding model using llama-nemotron-embed-vl-1b-v2.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/llama-nemotron-embed-vl-1b-v2\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Loading embedding model: {model_name}...\")\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"eager\",  # Use standard attention (Flash Attention not required)\n",
    "        ).eval()\n",
    "        \n",
    "        # Force eager attention on nested language model (fix for Flash Attention not installed)\n",
    "        if hasattr(self.model, 'language_model') and hasattr(self.model.language_model, 'config'):\n",
    "            self.model.language_model.config._attn_implementation = \"eager\"\n",
    "        \n",
    "        # Configure processor for different modalities\n",
    "        self.model.processor.p_max_length = 8192  # Text-only default\n",
    "        self.model.processor.max_input_tiles = 6\n",
    "        self.model.processor.use_thumbnail = True\n",
    "        print(\"âœ… Embedding model loaded!\")\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a text query.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_queries([text])\n",
    "            return embeddings[0].cpu().float().numpy().tolist()\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed text documents.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_documents(texts=texts)\n",
    "            return embeddings.cpu().float().numpy().tolist()\n",
    "    \n",
    "    def embed_images(self, images: List[Image.Image]) -> List[List[float]]:\n",
    "        \"\"\"Embed document images.\"\"\"\n",
    "        self.model.processor.p_max_length = 2048  # Image mode\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_documents(images=images)\n",
    "        self.model.processor.p_max_length = 8192  # Reset\n",
    "        return embeddings.cpu().float().numpy().tolist()\n",
    "    \n",
    "    def embed_image_text_pairs(self, images: List[Image.Image], texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed image + text pairs for maximum retrieval accuracy.\"\"\"\n",
    "        self.model.processor.p_max_length = 10240  # Image+text mode\n",
    "        with torch.inference_mode():\n",
    "            embeddings = self.model.encode_documents(images=images, texts=texts)\n",
    "        self.model.processor.p_max_length = 8192  # Reset\n",
    "        return embeddings.cpu().float().numpy().tolist()\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings_model = NemotronVLEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the Reranking Model\n",
    "\n",
    "Initial retrieval casts a wide net using fast vector similarity. The reranker then performs **deeper query-document interaction** to surface the most relevant results.\n",
    "\n",
    "**Why rerank?** Embedding-based retrieval is fast but approximate. The reranker reads each candidate document alongside the query, enabling cross-attention between them. This improves accuracy by ~6-7% on benchmarks.\n",
    "\n",
    "The `llama-nemotron-rerank-vl-1b-v2` model handles both text and image documents, using the same multimodal architecture as the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reranking model: nvidia/llama-nemotron-rerank-vl-1b-v2...\n",
      "âœ… Reranking model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoProcessor\n",
    "\n",
    "class NemotronVLReranker:\n",
    "    \"\"\"Multimodal reranker using llama-nemotron-rerank-vl-1b-v2.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/llama-nemotron-rerank-vl-1b-v2\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Loading reranking model: {model_name}...\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"eager\",  # Use standard attention (Flash Attention not required)\n",
    "        ).eval()\n",
    "        \n",
    "        # Force eager attention on nested language model (fix for Flash Attention not installed)\n",
    "        # For SequenceClassification models, the path is model.model.language_model\n",
    "        if hasattr(self.model, 'model') and hasattr(self.model.model, 'language_model'):\n",
    "            self.model.model.language_model.config._attn_implementation = \"eager\"\n",
    "        \n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            max_input_tiles=6,\n",
    "            use_thumbnail=True,\n",
    "            rerank_max_length=8192\n",
    "        )\n",
    "        print(\"âœ… Reranking model loaded!\")\n",
    "    \n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[dict],  # [{\"text\": ..., \"image\": ...}]\n",
    "        top_k: int = 5\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"Rerank documents by relevance to the query.\"\"\"\n",
    "        \n",
    "        # Build examples for the reranker\n",
    "        examples = []\n",
    "        for doc in documents:\n",
    "            example = {\n",
    "                \"question\": query,\n",
    "                \"doc_text\": doc.get(\"text\", \"\"),\n",
    "                \"doc_image\": doc.get(\"image\", \"\")\n",
    "            }\n",
    "            examples.append(example)\n",
    "        \n",
    "        # Process and run inference\n",
    "        batch_dict = self.processor.process_queries_documents_crossencoder(examples)\n",
    "        batch_dict = {\n",
    "            k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in batch_dict.items()\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**batch_dict, return_dict=True)\n",
    "        \n",
    "        # Get scores and sort\n",
    "        logits = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "        scored_docs = [(doc, score) for doc, score in zip(documents, logits)]\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [doc for doc, _ in scored_docs[:top_k]]\n",
    "\n",
    "# Initialize the reranker\n",
    "reranker = NemotronVLReranker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build a Sample Knowledge Base\n",
    "\n",
    "Let's create a small knowledge base with both text and images to demonstrate multimodal retrieval. In production, you would index your actual documents, PDFs, and images here.\n",
    "\n",
    "**Sample topics:**\n",
    "- NVIDIA Isaac Lab (robotics)\n",
    "- Autonomous vehicles (NVIDIA DRIVE)\n",
    "- Nemotron 3 Nano architecture\n",
    "- Genomics research (Evo-2)\n",
    "- RAG fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 5 documents\n",
      "   - With images: 3\n",
      "   - Text only: 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents (text + images)\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"doc_1\",\n",
    "        \"text\": \"NVIDIA Isaac Lab is a unified framework for robot learning built on Isaac Sim. It provides modular components for locomotion, manipulation, and navigation tasks.\",\n",
    "        \"image_url\": \"https://developer.download.nvidia.com/images/isaac/nvidia-isaac-lab-1920x1080.jpg\",\n",
    "        \"has_image\": True\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_2\", \n",
    "        \"text\": \"Autonomous vehicles use AI for perception, planning, and control. NVIDIA DRIVE provides the compute platform for Level 4 autonomous driving.\",\n",
    "        \"image_url\": \"https://blogs.nvidia.com/wp-content/uploads/2018/01/automotive-key-visual-corp-blog-level4-av-og-1280x680-1.png\",\n",
    "        \"has_image\": True\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_3\",\n",
    "        \"text\": \"Nemotron 3 Nano is a family of efficient language models with up to 1M token context. It uses a Mamba-Transformer hybrid architecture for computational efficiency.\",\n",
    "        \"image_url\": None,\n",
    "        \"has_image\": False\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_4\",\n",
    "        \"text\": \"NVIDIA Evo-2 is a biological foundation model for analyzing DNA, RNA, and protein sequences. It enables genomic research and drug discovery applications.\",\n",
    "        \"image_url\": \"https://developer-blogs.nvidia.com/wp-content/uploads/2025/02/hc-press-evo2-nim-25-featured-b.jpg\",\n",
    "        \"has_image\": True\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_5\",\n",
    "        \"text\": \"RAG (Retrieval Augmented Generation) improves LLM accuracy by grounding responses in retrieved documents. It reduces hallucinations and enables knowledge updates without retraining.\",\n",
    "        \"image_url\": None,\n",
    "        \"has_image\": False\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load images\n",
    "def load_image_from_url(url: str) -> Optional[Image.Image]:\n",
    "    \"\"\"Load an image from a URL.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all document images\n",
    "for doc in sample_documents:\n",
    "    doc[\"image\"] = load_image_from_url(doc.get(\"image_url\"))\n",
    "\n",
    "print(f\"âœ… Loaded {len(sample_documents)} documents\")\n",
    "print(f\"   - With images: {sum(1 for d in sample_documents if d['image'] is not None)}\")\n",
    "print(f\"   - Text only: {sum(1 for d in sample_documents if d['image'] is None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document embeddings...\n",
      "âœ… Created embeddings with shape: (5, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for all documents\n",
    "print(\"Creating document embeddings...\")\n",
    "\n",
    "document_embeddings = []\n",
    "for doc in sample_documents:\n",
    "    if doc[\"image\"] is not None:\n",
    "        # Use image+text embedding for maximum accuracy\n",
    "        emb = embeddings_model.embed_image_text_pairs([doc[\"image\"]], [doc[\"text\"]])[0]\n",
    "    else:\n",
    "        # Text-only embedding\n",
    "        emb = embeddings_model.embed_documents([doc[\"text\"]])[0]\n",
    "    document_embeddings.append(emb)\n",
    "\n",
    "document_embeddings = np.array(document_embeddings, dtype=np.float32)\n",
    "print(f\"âœ… Created embeddings with shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS index created with 5 vectors\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS index for fast similarity search\n",
    "import faiss\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(document_embeddings)\n",
    "\n",
    "# Create index\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product = cosine similarity after normalization\n",
    "index.add(document_embeddings)\n",
    "\n",
    "print(f\"âœ… FAISS index created with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How is AI used in robotics?\n",
      "\n",
      "Top 2 results:\n",
      "  1. NVIDIA Isaac Lab is a unified framework for robot learning built on Isaac Sim. It provides modular c...\n",
      "     Has image: True\n",
      "  2. Autonomous vehicles use AI for perception, planning, and control. NVIDIA DRIVE provides the compute ...\n",
      "     Has image: True\n"
     ]
    }
   ],
   "source": [
    "def retrieve_and_rerank(query: str, top_k: int = 3) -> List[dict]:\n",
    "    \"\"\"Retrieve documents and rerank them for a given query.\"\"\"\n",
    "    \n",
    "    # Step 1: Embed the query\n",
    "    query_embedding = np.array([embeddings_model.embed_query(query)], dtype=np.float32)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Step 2: Search the index (retrieve more than we need for reranking)\n",
    "    k_retrieve = min(len(sample_documents), top_k * 2)\n",
    "    scores, indices = index.search(query_embedding, k_retrieve)\n",
    "    \n",
    "    # Step 3: Get candidate documents\n",
    "    candidates = [sample_documents[i] for i in indices[0]]\n",
    "    \n",
    "    # Step 4: Rerank candidates\n",
    "    reranked = reranker.rerank(query, candidates, top_k=top_k)\n",
    "    \n",
    "    return reranked\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"How is AI used in robotics?\"\n",
    "results = retrieve_and_rerank(test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nTop {len(results)} results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. {doc['text'][:100]}...\")\n",
    "    print(f\"     Has image: {doc['image'] is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Add Real-Time Speech with Nemotron Speech ASR\n",
    "\n",
    "Now we add voice input capability. The `nemotron-speech-streaming-en-0.6b` model converts spoken audio to text with ultra-low latency.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ASR PIPELINE                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚   â”‚  Audio   â”‚â”€â”€â”€>â”‚  NeMo    â”‚â”€â”€â”€>â”‚  Text    â”‚              â”‚\n",
    "â”‚   â”‚  Stream  â”‚    â”‚  ASR     â”‚    â”‚  Output  â”‚              â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                        â”‚                                    â”‚\n",
    "â”‚                        v                                    â”‚\n",
    "â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n",
    "â”‚              â”‚ + Punctuation   â”‚                            â”‚\n",
    "â”‚              â”‚ + Capitalizationâ”‚                            â”‚\n",
    "â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key features:**\n",
    "- Trained on 285k-hour Granary dataset\n",
    "- 7.16% average WER on Open ASR Leaderboard\n",
    "- Cache-aware streaming for real-time applications\n",
    "- Built-in punctuation and capitalization\n",
    "- Configurable latency: 80ms to 1.1s chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-06 00:00:29 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.\n",
      "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "No exporters were provided. This means that no telemetry data will be collected.\n",
      "[NeMo W 2026-01-06 00:00:30 nemo_logging:405] /home/chris/projects/use-case-examples/nemotron-voice-rag-agent-example/.venv/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASR model: nvidia/nemotron-speech-streaming-en-0.6b...\n",
      "[NeMo I 2026-01-06 00:00:32 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-06 00:00:32 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: /lustre/fs12/portfolios/llmservice/projects/llmservice_nemo_speechlm/users/weiqingw/manifests/input_cfg/am-os_fl_ll_mc_mm_mo_no_su_yo_yt_gsc_en.yaml\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    use_bucketing: true\n",
      "    max_tps:\n",
      "    - 10.92\n",
      "    - 11.16\n",
      "    - 10.68\n",
      "    - 10.22\n",
      "    - 9.98\n",
      "    - 9.67\n",
      "    - 9.5\n",
      "    - 9.36\n",
      "    - 9.04\n",
      "    - 9.38\n",
      "    - 8.81\n",
      "    - 8.78\n",
      "    - 8.24\n",
      "    - 8.85\n",
      "    - 9.25\n",
      "    bucket_duration_bins:\n",
      "    - - 5.76\n",
      "      - 62\n",
      "    - - 7.12\n",
      "      - 77\n",
      "    - - 8.32\n",
      "      - 83\n",
      "    - - 9.44\n",
      "      - 92\n",
      "    - - 10.5\n",
      "      - 103\n",
      "    - - 11.68\n",
      "      - 111\n",
      "    - - 12.88\n",
      "      - 117\n",
      "    - - 14.08\n",
      "      - 130\n",
      "    - - 15.44\n",
      "      - 138\n",
      "    - - 17.2\n",
      "      - 156\n",
      "    - - 19.36\n",
      "      - 158\n",
      "    - - 22.4\n",
      "      - 189\n",
      "    - - 26.64\n",
      "      - 217\n",
      "    - - 32.8\n",
      "      - 272\n",
      "    - - 40.1\n",
      "      - 352\n",
      "    bucket_batch_size:\n",
      "    - 100\n",
      "    - 100\n",
      "    - 80\n",
      "    - 80\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 30\n",
      "    - 20\n",
      "    - 20\n",
      "    - 15\n",
      "    - 10\n",
      "    - 3\n",
      "    num_buckets: 15\n",
      "    bucket_buffer_size: 7500\n",
      "    shuffle_buffer_size: 5000\n",
      "    \n",
      "[NeMo W 2026-01-06 00:00:32 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath:\n",
      "    - /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/canary/canary_v0/manifests/data/ASR/MMLPC/en/val_test/mcv11/mcv11_dev_clean_pcstrip_en_2k.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-06 00:00:32 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2026-01-06 00:00:34 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-06 00:00:34 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-06 00:00:34 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.005, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-06 00:00:35 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from /home/chris/.cache/huggingface/hub/models--nvidia--nemotron-speech-streaming-en-0.6b/snapshots/e730059607cecd9cccf501d8a39f5d22f0993db8/nemotron-speech-streaming-en-0.6b.nemo.\n",
      "  â†’ Disabled CUDA graphs on decoding_computer\n",
      "âœ… ASR model loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from omegaconf import OmegaConf\n",
    "from typing import List\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class NemotronASR:\n",
    "    \"\"\"Speech-to-text using Nemotron Speech Streaming model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/nemotron-speech-streaming-en-0.6b\"):\n",
    "        print(f\"Loading ASR model: {model_name}...\")\n",
    "        self.model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name)\n",
    "        self.model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        self._disable_cuda_graphs()\n",
    "        print(\"âœ… ASR model loaded!\")\n",
    "    \n",
    "    def _disable_cuda_graphs(self):\n",
    "        \"\"\"Disable CUDA graphs in the decoding pipeline.\"\"\"\n",
    "        try:\n",
    "            if hasattr(self.model, 'decoding') and hasattr(self.model.decoding, 'decoding'):\n",
    "                decoder = self.model.decoding.decoding\n",
    "                if hasattr(decoder, 'decoding_computer') and decoder.decoding_computer is not None:\n",
    "                    decoder.decoding_computer.cuda_graphs_mode = None\n",
    "                    print(\"  â†’ Disabled CUDA graphs on decoding_computer\")\n",
    "                    return\n",
    "        except Exception as e:\n",
    "            print(f\"  â†’ Direct patch failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            if hasattr(self.model, 'cfg') and hasattr(self.model.cfg, 'decoding'):\n",
    "                decoding_cfg = OmegaConf.to_container(self.model.cfg.decoding, resolve=True)\n",
    "                decoding_cfg = OmegaConf.create(decoding_cfg)\n",
    "                OmegaConf.set_struct(decoding_cfg, False)\n",
    "                \n",
    "                if 'greedy' in decoding_cfg and decoding_cfg.greedy is not None:\n",
    "                    decoding_cfg.greedy.cuda_graphs_mode = None\n",
    "                \n",
    "                self.model.change_decoding_strategy(decoding_cfg)\n",
    "                \n",
    "                if hasattr(self.model.decoding, 'decoding'):\n",
    "                    decoder = self.model.decoding.decoding\n",
    "                    if hasattr(decoder, 'decoding_computer') and decoder.decoding_computer is not None:\n",
    "                        decoder.decoding_computer.cuda_graphs_mode = None\n",
    "                \n",
    "                print(\"  â†’ Rebuilt decoding strategy with CUDA graphs disabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"  â†’ Config rebuild failed: {e}\")\n",
    "    \n",
    "    def _to_mono(self, audio_path: str) -> str:\n",
    "        \"\"\"Convert audio to mono if needed, return path to mono file.\"\"\"\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        \n",
    "        # Already mono\n",
    "        if len(audio.shape) == 1 or audio.shape[1] == 1:\n",
    "            return audio_path\n",
    "        \n",
    "        # Convert to mono by averaging channels\n",
    "        audio_mono = np.mean(audio, axis=1)\n",
    "        \n",
    "        # Write to temp file\n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
    "        sf.write(temp_file.name, audio_mono, sr)\n",
    "        return temp_file.name\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> str:\n",
    "        \"\"\"Transcribe an audio file to text.\"\"\"\n",
    "        mono_path = self._to_mono(audio_path)\n",
    "        try:\n",
    "            transcriptions = self.model.transcribe([mono_path])\n",
    "            return transcriptions[0] if transcriptions else \"\"\n",
    "        finally:\n",
    "            # Clean up temp file if we created one\n",
    "            if mono_path != audio_path:\n",
    "                os.unlink(mono_path)\n",
    "    \n",
    "    def transcribe_batch(self, audio_paths: List[str]) -> List[str]:\n",
    "        \"\"\"Transcribe multiple audio files.\"\"\"\n",
    "        mono_paths = [self._to_mono(p) for p in audio_paths]\n",
    "        try:\n",
    "            return self.model.transcribe(mono_paths)\n",
    "        finally:\n",
    "            for mono_path, orig_path in zip(mono_paths, audio_paths):\n",
    "                if mono_path != orig_path:\n",
    "                    os.unlink(mono_path)\n",
    "\n",
    "asr_model = NemotronASR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-06 00:00:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2026-01-06 00:00:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "Transcribing: 1it [00:00, 12.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Transcription: Hypothesis(score=-465.7001953125, y_sequence=tensor([112, 127,  41, 685, 342, 291,  32, 120, 143, 160, 358, 963,  54, 589,\n",
      "        977]), text='Could you please tell me about robotics?', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download a sample audio file for testing\n",
    "import urllib.request\n",
    "\n",
    "sample_audio_path = \"robotics.flac\"\n",
    "\n",
    "# Transcribe the sample\n",
    "transcription = asr_model.transcribe(sample_audio_path)\n",
    "print(f\"\\nğŸ“ Transcription: {transcription}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Enforce Safety with Nemotron Content Safety and PII Detection\n",
    "\n",
    "Production agents need guardrails. The `Llama-3.1-Nemotron-Safety-Guard-8B-v3` model checks both user inputs and agent outputs for safety violations.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    SAFETY PIPELINE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚   â”‚  User    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                  â”‚                 â”‚\n",
    "â”‚   â”‚  Query   â”‚         â”‚   Safety Guard   â”‚â”€â”€> Safe/Unsafe  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                  â”‚                 â”‚\n",
    "â”‚                        â”‚  23 Categories:  â”‚                 â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  - Violence      â”‚                 â”‚\n",
    "â”‚   â”‚  Agent   â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  - PII/Privacy   â”‚                 â”‚\n",
    "â”‚   â”‚ Response â”‚         â”‚  - Harassment    â”‚                 â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - Fraud         â”‚                 â”‚\n",
    "â”‚                        â”‚  - Malware       â”‚                 â”‚\n",
    "â”‚                        â”‚  - etc.          â”‚                 â”‚\n",
    "â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key features:**\n",
    "- Multilingual support (20+ languages)\n",
    "- PII detection (emails, SSNs, phone numbers)\n",
    "- Cultural context awareness\n",
    "- 23 safety categories\n",
    "- Works with noisy ASR output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading safety model: nvidia/Llama-3.1-Nemotron-Safety-Guard-8B-v3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b71deeb60724ea59a3777a2a73411a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Safety model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from jinja2 import Template\n",
    "import json\n",
    "\n",
    "class NemotronSafetyGuard:\n",
    "    \"\"\"Content safety and PII detection using Llama-3.1-Nemotron-Safety-Guard-8B-v3.\"\"\"\n",
    "    \n",
    "    SAFETY_TEMPLATE = Template(\"\"\"Task: Check if there is unsafe content in the user or agent messages in the below conversation in accordance with our safety policy with the below categories. Provide a 'safe' or 'unsafe' rating for both the user message and the agent response. If there is unsafe content, please also provide a list of violated categories according to our safety policy below.\n",
    "\n",
    "<BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "S1: Violence. \n",
    "S2: Sexual. \n",
    "S3: Criminal Planning/Confessions. \n",
    "S4: Guns and Illegal Weapons. \n",
    "S5: Controlled/Regulated Substances. \n",
    "S6: Suicide and Self Harm. \n",
    "S7: Sexual (minor). \n",
    "S8: Hate/Identity Hate. \n",
    "S9: PII/Privacy. \n",
    "S10: Harassment. \n",
    "S11: Threat. \n",
    "S12: Profanity. \n",
    "S13: Needs Caution. \n",
    "S14: Other. \n",
    "S15: Manipulation. \n",
    "S16: Fraud/Deception. \n",
    "S17: Malware. \n",
    "S18: High Risk Gov Decision Making. \n",
    "S19: Political/Misinformation/Conspiracy. \n",
    "S20: Copyright/Trademark/Plagiarism. \n",
    "S21: Unauthorized Advice. \n",
    "S22: Illegal Activity. \n",
    "S23: Immoral/Unethical. \n",
    "<END UNSAFE CONTENT CATEGORIES>\n",
    "\n",
    "<BEGIN CONVERSATION>\n",
    "\n",
    "user: {{ query }}\n",
    "{% if response %}\n",
    "response: agent: {{ response }}\n",
    "{% endif %}\n",
    "<END CONVERSATION>\n",
    "\n",
    "Provide your safety assessment for the above conversation in the below output JSON format:\n",
    "{\"User Safety\": safe or unsafe rating for the user message, \"Response Safety\": safe or unsafe rating for the agent response. Omit if no agent response present. \"Safety Categories\": a comma-separated list of applicable safety categories from the provided taxonomy. Omit if all safe.}\n",
    "\n",
    "Do not include anything other than the output JSON in your response.\n",
    "Output JSON: \"\"\")\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nvidia/Llama-3.1-Nemotron-Safety-Guard-8B-v3\"):\n",
    "        print(f\"Loading safety model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        ).eval()\n",
    "        print(\"âœ… Safety model loaded!\")\n",
    "    \n",
    "    def check_safety(self, query: str, response: Optional[str] = None) -> dict:\n",
    "        \"\"\"Check content safety for a query and optional response.\"\"\"\n",
    "        \n",
    "        # Construct the prompt\n",
    "        constructed_prompt = self.SAFETY_TEMPLATE.render(query=query, response=response)\n",
    "        \n",
    "        # Apply chat template\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": constructed_prompt}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "        result = self.tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            return json.loads(result)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"User Safety\": \"unknown\", \"raw_response\": result}\n",
    "    \n",
    "    def is_safe(self, query: str, response: Optional[str] = None) -> bool:\n",
    "        \"\"\"Quick check if content is safe.\"\"\"\n",
    "        result = self.check_safety(query, response)\n",
    "        user_safe = result.get(\"User Safety\", \"unsafe\").lower() == \"safe\"\n",
    "        response_safe = result.get(\"Response Safety\", \"safe\").lower() == \"safe\"\n",
    "        return user_safe and response_safe\n",
    "\n",
    "# Initialize safety guard\n",
    "safety_guard = NemotronSafetyGuard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›¡ï¸ Safety Check Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Query: How does AI improve robotics?...\n",
      "   Result: {'User Safety': 'safe', 'Response Safety': 'safe'}\n",
      "\n",
      "2. Query: My email is john@example.com and my SSN is 123-45-...\n",
      "   Result: {'User Safety': 'unsafe', 'Safety Categories': 'PII/Privacy'}\n"
     ]
    }
   ],
   "source": [
    "# Test safety checking\n",
    "test_cases = [\n",
    "    {\"query\": \"How does AI improve robotics?\", \"response\": \"AI enables robots to perceive and act autonomously.\"},\n",
    "    {\"query\": \"My email is john@example.com and my SSN is 123-45-6789\", \"response\": None},\n",
    "]\n",
    "\n",
    "print(\"ğŸ›¡ï¸ Safety Check Results:\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    result = safety_guard.check_safety(test[\"query\"], test.get(\"response\"))\n",
    "    print(f\"\\n{i}. Query: {test['query'][:50]}...\")\n",
    "    print(f\"   Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Add Long-Context Reasoning with Nemotron 3 Nano\n",
    "\n",
    "With retrieval, speech, and safety in place, we add the reasoning engine. **Nemotron 3 Nano** processes the retrieved context and generates intelligent responses.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 REASONING PIPELINE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
    "â”‚   â”‚ Retrievedâ”‚â”€â”€â”€â”€â”€â”                                        â”‚\n",
    "â”‚   â”‚   Docs   â”‚     â”‚                                        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚                    â”œâ”€â”€â”€>â”‚                â”‚                  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚  Nemotron 3    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Image   â”‚â”€â”€â”€â”€â”€â”¤    â”‚     Nano       â”‚â”€â”€â”€>â”‚ Response â”‚  â”‚\n",
    "â”‚   â”‚  Descs   â”‚     â”‚    â”‚                â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚  1M tokens     â”‚                  â”‚\n",
    "â”‚                    â”‚    â”‚  Mamba+Trans   â”‚                  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚   â”‚   User   â”‚â”€â”€â”€â”€â”€â”˜           â”‚                            â”‚\n",
    "â”‚   â”‚  Query   â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  Optional   â”‚                     â”‚\n",
    "â”‚                         â”‚  Thinking   â”‚                     â”‚\n",
    "â”‚                         â”‚    Mode     â”‚                     â”‚\n",
    "â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Architecture highlights:**\n",
    "- **1M token context**: Fit entire document collections in a single request\n",
    "- **Mamba-Transformer hybrid**: Efficient inference on long sequences\n",
    "- **Thinking mode**: Optional step-by-step reasoning for complex queries\n",
    "\n",
    "For images in retrieved documents, we first use **Nemotron Nano VL** to describe them, then include those descriptions in the context for Nemotron 3 Nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Nemotron LLM initialized!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "class NemotronLLM:\n",
    "    \"\"\"Wrapper for Nemotron models via NVIDIA API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        model: str = \"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "        enable_thinking: bool = True,\n",
    "        max_tokens: int = 4096\n",
    "    ) -> str:\n",
    "        \"\"\"Generate a response using Nemotron 3 Nano.\"\"\"\n",
    "        \n",
    "        extra_body = {}\n",
    "        if enable_thinking:\n",
    "            extra_body = {\n",
    "                \"reasoning_budget\": 4096,\n",
    "                \"chat_template_kwargs\": {\"enable_thinking\": True}\n",
    "            }\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=max_tokens,\n",
    "            extra_body=extra_body if extra_body else None,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def describe_image(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        query: str = \"Describe this image in detail.\"\n",
    "    ) -> str:\n",
    "        \"\"\"Describe an image using Nemotron Nano VL.\"\"\"\n",
    "        \n",
    "        # Convert image to base64\n",
    "        from io import BytesIO\n",
    "        buffer = BytesIO()\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "        base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"/no_think\"  # Fast mode without extended reasoning\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# Initialize LLM\n",
    "llm = NemotronLLM(NVIDIA_API_KEY)\n",
    "print(\"âœ… Nemotron LLM initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Response: \n",
      "Isaacâ€¯Lab is used as a unified framework for robot learning that enables the development and testing of AIâ€‘driven behaviorsâ€”such as locomotion, manipulation, and navigationâ€”through modular components built on NVIDIA Isaacâ€¯Sim.\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM\n",
    "test_prompt = \"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "NVIDIA Isaac Lab is a unified framework for robot learning built on Isaac Sim. \n",
    "It provides modular components for locomotion, manipulation, and navigation tasks.\n",
    "\n",
    "Question: What is Isaac Lab used for?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = llm.generate(test_prompt, enable_thinking=False)\n",
    "print(f\"ğŸ¤– Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Build a Voice-Powered LangChain v1.0 Agent with RAG\n",
    "\n",
    "Now we'll create a **LangChain v1.0 agent** using the modern `create_agent` API. The agent automatically loops, calling the RAG tool as needed until it can answer your voice query.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   LANGCHAIN v1.0 AGENT PIPELINE                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   ğŸ¤ Voice Input (Microphone)                                               |\n",
    "â”‚       â”‚                                                                     â”‚\n",
    "â”‚       v                                                                     â”‚\n",
    "â”‚   ASR (Nemotron) â†’ Text Query                                               â”‚\n",
    "â”‚       â”‚                                                                     â”‚\n",
    "â”‚       v                                                                     â”‚\n",
    "â”‚   ğŸ›¡ï¸ STEP 2: Safety Check (Input) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> âŒ REJECT         â”‚\n",
    "â”‚       â”‚                                                                     â”‚\n",
    "â”‚       v (if safe)                                                           â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚   â”‚  STEP 3: create_agent Loop (CompiledStateGraph)                â”‚        â”‚\n",
    "â”‚   â”‚                                                                â”‚        â”‚\n",
    "â”‚   â”‚  Agent (ChatNVIDIA) â”€â”€> Decides: Need RAG?                     â”‚        â”‚\n",
    "â”‚   â”‚     â”‚                â”‚                                         â”‚        â”‚\n",
    "â”‚   â”‚     NO               YES                                       â”‚        â”‚\n",
    "â”‚   â”‚     â”‚                â”‚                                         â”‚        â”‚\n",
    "â”‚   â”‚     â”‚                v                                         â”‚        â”‚\n",
    "â”‚   â”‚     â”‚           RAG Tool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚           â”‚                               â”‚              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚           v                               â”‚              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚       Has Images? â”€â”€YESâ”€â”€> VLM Describe   â”‚              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚           â”‚                    â”‚          â”‚              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚           NO                   v          â”‚              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚           â”‚           Add to Context      â”‚              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚        â”‚\n",
    "â”‚   â”‚     â”‚                                   â”‚                      â”‚        â”‚\n",
    "â”‚   â”‚     v                                   v                      â”‚        â”‚\n",
    "â”‚   â”‚  Generate Response <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚        â”‚\n",
    "â”‚   â”‚                                                                â”‚        â”‚\n",
    "â”‚   â”‚  (Loop continues until agent is satisfied)                     â”‚        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚       â”‚                                                                     â”‚\n",
    "â”‚       v                                                                     â”‚\n",
    "â”‚   ğŸ›¡ï¸ STEP 4: Safety Check (Output) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> âŒ FILTER        â”‚\n",
    "â”‚       â”‚                                                                     â”‚\n",
    "â”‚       v (if safe)                                                           â”‚\n",
    "â”‚   âœ… STEP 5: Return Safe Response                                           â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Uses `langchain.agents.create_agent` (LangChain v1.0 API)\n",
    "- Returns `CompiledStateGraph` that auto-loops until complete\n",
    "- Integrates with `ChatNVIDIA` for NVIDIA API endpoints\n",
    "- Uses VLM when images are retrieved in context\n",
    "- Safety ALWAYS enforced on input AND output\n",
    "\n",
    "First, define the RAG tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG tool defined!\n",
      "   Function: search_knowledge_base\n",
      "   Capabilities: Multimodal retrieval + reranking + image description\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define RAG tool\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the knowledge base for information about NVIDIA technologies, robotics, AI, and related topics.\n",
    "    \n",
    "    Use this tool when you need factual information about:\n",
    "    - NVIDIA Isaac Lab and robotics\n",
    "    - Autonomous vehicles and NVIDIA DRIVE  \n",
    "    - Nemotron models and architecture\n",
    "    - Genomics research (Evo-2)\n",
    "    - RAG fundamentals\n",
    "    - Any other technical topics in the knowledge base\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant documents\n",
    "        \n",
    "    Returns:\n",
    "        Relevant information from the knowledge base with image descriptions\n",
    "    \"\"\"\n",
    "    # Retrieve and rerank documents\n",
    "    docs = retrieve_and_rerank(query, top_k=3)\n",
    "    \n",
    "    # Build context with text\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        context_parts.append(f\"{i}. {doc['text']}\")\n",
    "    \n",
    "    # Describe images if present\n",
    "    for doc in docs:\n",
    "        if doc.get(\"image\") is not None:\n",
    "            desc = llm.describe_image(\n",
    "                doc[\"image\"],\n",
    "                f\"Describe this image in the context of: {query}\"\n",
    "            )\n",
    "            context_parts.append(f\"\\n[Image from {doc['id']}]: {desc}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "print(\"âœ… RAG tool defined!\")\n",
    "print(\"   Function: search_knowledge_base\")\n",
    "print(\"   Capabilities: Multimodal retrieval + reranking + image description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain v1.0 Agent created!\n",
      "   API: langchain.agents.create_agent (LangChain v1.0)\n",
      "   Model: nvidia/nemotron-3-nano-30b-a3b via ChatNVIDIA\n",
      "   Tools: search_knowledge_base\n",
      "   Returns: CompiledStateGraph (auto-loops until complete)\n"
     ]
    }
   ],
   "source": [
    "# Create LangChain v1.0 Agent using the modern create_agent API\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import warnings\n",
    "\n",
    "# Suppress the type warning for custom NVIDIA models\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found nvidia/nemotron-3-nano-30b-a3b\")\n",
    "\n",
    "# Configure the LLM for the agent using ChatNVIDIA\n",
    "agent_llm = ChatNVIDIA(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    api_key=NVIDIA_API_KEY,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# System prompt for the agent\n",
    "AGENT_SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant with access to a knowledge base about NVIDIA technologies, AI, and robotics.\n",
    "\n",
    "IMPORTANT: When the user asks a question that requires factual information about:\n",
    "- NVIDIA Isaac Lab or robotics\n",
    "- Autonomous vehicles or NVIDIA DRIVE\n",
    "- Nemotron models and architecture\n",
    "- Genomics research (Evo-2)\n",
    "- RAG fundamentals\n",
    "- Any other technical topics\n",
    "\n",
    "You MUST use the search_knowledge_base tool to retrieve accurate information.\n",
    "Do NOT make up information - always search first for factual queries.\n",
    "\n",
    "For simple greetings or conversational responses, you may respond directly.\n",
    "\n",
    "Always be concise, helpful, and ground your answers in the retrieved information.\"\"\"\n",
    "\n",
    "# Create the agent using LangChain v1.0 create_agent API\n",
    "# This returns a CompiledStateGraph that automatically loops until done\n",
    "agent_graph = create_agent(\n",
    "    model=agent_llm,\n",
    "    tools=[search_knowledge_base],\n",
    "    system_prompt=AGENT_SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"âœ… LangChain v1.0 Agent created!\")\n",
    "print(f\"   API: langchain.agents.create_agent (LangChain v1.0)\")\n",
    "print(f\"   Model: nvidia/nemotron-3-nano-30b-a3b via ChatNVIDIA\")\n",
    "print(f\"   Tools: search_knowledge_base\")\n",
    "print(f\"   Returns: CompiledStateGraph (auto-loops until complete)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Voice RAG Agent initialized!\n",
      "\n",
      "Pipeline Flow:\n",
      "  1. ğŸ¤ Voice Input â†’ Nemotron ASR â†’ Text\n",
      "  2. ğŸ›¡ï¸ Safety Check (input) â†’ Reject if unsafe\n",
      "  3. ğŸ¤– Agent Loop (LangChain v1.0 create_agent)\n",
      "     â””â”€ RAG Tool â†’ Embed + Rerank + VLM (if images)\n",
      "  4. ğŸ›¡ï¸ Safety Check (output) â†’ Filter if unsafe\n",
      "  5. âœ… Return safe response\n"
     ]
    }
   ],
   "source": [
    "# Create Voice Agent Wrapper with comprehensive safety pipeline\n",
    "class VoiceRAGAgent:\n",
    "    \"\"\"\n",
    "    Voice-powered RAG agent with mandatory safety checks.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Voice Input â†’ ASR transcription\n",
    "    2. Input Safety Check â†’ Reject if unsafe\n",
    "    3. Agent Loop (with RAG tool as needed)\n",
    "       - If image in context â†’ VLM describes it\n",
    "       - LLM reasons over context\n",
    "    4. Output Safety Check â†’ Filter if unsafe\n",
    "    5. Return safe response\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, agent_graph, asr_model, safety_guard, vlm_model=None):\n",
    "        self.agent = agent_graph  # CompiledStateGraph from create_agent\n",
    "        self.asr = asr_model\n",
    "        self.safety = safety_guard\n",
    "        self.vlm = vlm_model  # NemotronLLM for image descriptions\n",
    "    \n",
    "    def _extract_response(self, result: dict) -> str:\n",
    "        \"\"\"Extract the final response from agent result.\"\"\"\n",
    "        messages = result.get(\"messages\", [])\n",
    "        if not messages:\n",
    "            return \"\"\n",
    "        \n",
    "        # Get the last AI message\n",
    "        for msg in reversed(messages):\n",
    "            if hasattr(msg, 'content') and hasattr(msg, 'type'):\n",
    "                if msg.type == 'ai' and msg.content:\n",
    "                    return msg.content\n",
    "            elif isinstance(msg, dict) and msg.get('type') == 'ai':\n",
    "                return msg.get('content', '')\n",
    "        \n",
    "        # Fallback: return last message content\n",
    "        last_msg = messages[-1]\n",
    "        if hasattr(last_msg, 'content'):\n",
    "            return last_msg.content\n",
    "        return str(last_msg)\n",
    "    \n",
    "    def run_voice(self, audio_path: str, verbose: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Run the full voice RAG pipeline.\n",
    "        \n",
    "        Pipeline Steps:\n",
    "        1. ASR: Convert voice to text\n",
    "        2. Safety Check: Validate input is safe\n",
    "        3. Agent: Process with RAG tool (auto-loops until satisfied)\n",
    "           - Uses VLM if images are retrieved\n",
    "           - Uses LLM for reasoning\n",
    "        4. Safety Check: Validate output is safe\n",
    "        5. Return: Safe response\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            verbose: Print pipeline steps\n",
    "            \n",
    "        Returns:\n",
    "            dict with query, response, is_safe, and metadata\n",
    "        \"\"\"\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 1: Voice â†’ Text (ASR)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\"ğŸ¤ STEP 1: Voice Transcription (Nemotron ASR)\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        transcription = self.asr.transcribe(audio_path)\n",
    "        \n",
    "        # Handle different transcription output formats\n",
    "        if hasattr(transcription, 'text'):\n",
    "            query = transcription.text\n",
    "        else:\n",
    "            query = str(transcription)\n",
    "        \n",
    "        # Clean up query if it contains Hypothesis wrapper\n",
    "        if \"text='\" in query:\n",
    "            import re\n",
    "            match = re.search(r\"text='([^']+)'\", query)\n",
    "            if match:\n",
    "                query = match.group(1)\n",
    "        \n",
    "        if not query or not query.strip():\n",
    "            return {\"error\": \"No speech detected in audio\", \"is_safe\": False}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ“ Transcription: {query}\\n\")\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 2: Input Safety Check\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\"ğŸ›¡ï¸ STEP 2: Input Safety Validation\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        input_safety = self.safety.check_safety(query)\n",
    "        input_is_safe = input_safety.get(\"User Safety\", \"unsafe\").lower() == \"safe\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Safety Result: {input_safety}\")\n",
    "        \n",
    "        if not input_is_safe:\n",
    "            if verbose:\n",
    "                print(\"âŒ INPUT REJECTED: Safety violation detected\\n\")\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": \"I cannot process this request due to safety concerns.\",\n",
    "                \"is_safe\": False,\n",
    "                \"safety_details\": input_safety,\n",
    "                \"blocked_at\": \"input_validation\"\n",
    "            }\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"âœ… Input is safe - proceeding to agent\\n\")\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 3: Agent Processing (with RAG tool)\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\"ğŸ¤– STEP 3: Agent Processing (RAG Tool Available)\")\n",
    "            print(\"=\" * 70)\n",
    "            print(\"   Agent will automatically:\")\n",
    "            print(\"   - Decide if knowledge base search is needed\")\n",
    "            print(\"   - Call RAG tool to retrieve documents\")\n",
    "            print(\"   - Use VLM if images are in retrieved context\")\n",
    "            print(\"   - Generate response grounded in facts\")\n",
    "            print(\"-\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Invoke the agent with proper message format for LangChain v1.0\n",
    "            result = self.agent.invoke({\n",
    "                \"messages\": [(\"user\", query)]\n",
    "            })\n",
    "            \n",
    "            # Extract the final response\n",
    "            response = self._extract_response(result)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"-\" * 70)\n",
    "                print(f\"âœ… Agent completed successfully\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"âŒ Agent error: {e}\\n\")\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"error\": str(e),\n",
    "                \"is_safe\": False,\n",
    "                \"blocked_at\": \"agent_execution\"\n",
    "            }\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 4: Output Safety Check\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\"ğŸ›¡ï¸ STEP 4: Output Safety Validation\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        output_safety = self.safety.check_safety(query, response)\n",
    "        \n",
    "        user_safe = output_safety.get(\"User Safety\", \"unsafe\").lower() == \"safe\"\n",
    "        response_safe = output_safety.get(\"Response Safety\", \"safe\").lower() == \"safe\"\n",
    "        is_safe = user_safe and response_safe\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Safety Result: {output_safety}\")\n",
    "        \n",
    "        if not is_safe:\n",
    "            if verbose:\n",
    "                print(\"âŒ OUTPUT FILTERED: Safety violation in response\\n\")\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": \"The generated response was filtered due to safety concerns.\",\n",
    "                \"is_safe\": False,\n",
    "                \"safety_details\": output_safety,\n",
    "                \"blocked_at\": \"output_validation\"\n",
    "            }\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"âœ… Output is safe\\n\")\n",
    "        \n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        # STEP 5: Return Safe Response\n",
    "        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"is_safe\": True,\n",
    "            \"safety_details\": output_safety\n",
    "        }\n",
    "\n",
    "\n",
    "# Create the voice RAG agent with all components\n",
    "voice_agent = VoiceRAGAgent(\n",
    "    agent_graph=agent_graph,\n",
    "    asr_model=asr_model,\n",
    "    safety_guard=safety_guard,\n",
    "    vlm_model=llm  # NemotronLLM for VLM image description\n",
    ")\n",
    "\n",
    "print(\"âœ… Voice RAG Agent initialized!\")\n",
    "print()\n",
    "print(\"Pipeline Flow:\")\n",
    "print(\"  1. ğŸ¤ Voice Input â†’ Nemotron ASR â†’ Text\")\n",
    "print(\"  2. ğŸ›¡ï¸ Safety Check (input) â†’ Reject if unsafe\")\n",
    "print(\"  3. ğŸ¤– Agent Loop (LangChain v1.0 create_agent)\")\n",
    "print(\"     â””â”€ RAG Tool â†’ Embed + Rerank + VLM (if images)\")\n",
    "print(\"  4. ğŸ›¡ï¸ Safety Check (output) â†’ Filter if unsafe\")\n",
    "print(\"  5. âœ… Return safe response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interactive Voice Interface\n",
    "\n",
    "Now let's create an **interactive microphone recorder** to query the agent with your voice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“ OPTION 1: Upload Audio File (Recommended for SSH/Remote)\n",
      "======================================================================\n",
      "Upload a .wav, .mp3, .flac, .webm, or .ogg file from your local machine.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcaac527ab74290ba35ad74b2000a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.wav,.mp3,.flac,.webm,.ogg,.m4a', description='Upload Audio', layout=Layout(widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ™ï¸ OPTION 2: Browser Microphone (Local Jupyter Only)\n",
      "======================================================================\n",
      "âš ï¸  This requires localhost or HTTPS. Will show 'Permission denied' over SSH.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"padding: 20px; background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 12px; text-align: center; color: #eee; font-family: system-ui, -apple-system, sans-serif;\">\n",
       "    <h3 style=\"margin-top: 0; color: #00d4ff;\">ğŸ™ï¸ Browser Microphone</h3>\n",
       "    <p style=\"color: #aaa; font-size: 14px;\"><strong>Note:</strong> Only works on localhost or HTTPS connections</p>\n",
       "    <div id=\"controls\" style=\"margin: 20px 0;\">\n",
       "        <button id=\"recordBtn\" style=\"background: linear-gradient(135deg, #00b894 0%, #00cec9 100%); color: white; padding: 15px 30px; font-size: 16px; border: none; border-radius: 8px; margin: 5px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 15px rgba(0,184,148,0.3);\">\n",
       "            ğŸ™ï¸ Start Recording\n",
       "        </button>\n",
       "        <button id=\"stopBtn\" disabled style=\"background: linear-gradient(135deg, #e17055 0%, #d63031 100%); color: white; padding: 15px 30px; font-size: 16px; border: none; border-radius: 8px; margin: 5px; cursor: pointer; opacity: 0.5; font-weight: 600;\">\n",
       "            â¹ï¸ Stop Recording\n",
       "        </button>\n",
       "    </div>\n",
       "    <div id=\"status\" style=\"padding: 15px; font-weight: bold; color: #ffeaa7; background: rgba(255,255,255,0.1); border-radius: 8px; margin: 10px 0;\"></div>\n",
       "    <audio id=\"audioPlayback\" controls style=\"margin-top: 20px; display: none; width: 100%;\"></audio>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "let mediaRecorder;\n",
       "let audioChunks = [];\n",
       "\n",
       "const recordBtn = document.getElementById('recordBtn');\n",
       "const stopBtn = document.getElementById('stopBtn');\n",
       "const status = document.getElementById('status');\n",
       "const audioPlayback = document.getElementById('audioPlayback');\n",
       "\n",
       "// Check if we're on a secure context\n",
       "if (!window.isSecureContext) {\n",
       "    status.innerHTML = 'âš ï¸ <strong>Not a secure context.</strong> Microphone requires localhost or HTTPS.<br>ğŸ‘‰ Use the <strong>File Upload</strong> option above instead.';\n",
       "    status.style.color = '#fab1a0';\n",
       "    recordBtn.disabled = true;\n",
       "    recordBtn.style.opacity = '0.5';\n",
       "}\n",
       "\n",
       "recordBtn.onclick = async () => {\n",
       "    try {\n",
       "        status.textContent = 'ğŸ”„ Requesting microphone access...';\n",
       "        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
       "        mediaRecorder = new MediaRecorder(stream);\n",
       "        audioChunks = [];\n",
       "\n",
       "        mediaRecorder.ondataavailable = (event) => {\n",
       "            audioChunks.push(event.data);\n",
       "        };\n",
       "\n",
       "        mediaRecorder.onstop = () => {\n",
       "            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
       "            const audioUrl = URL.createObjectURL(audioBlob);\n",
       "            audioPlayback.src = audioUrl;\n",
       "            audioPlayback.style.display = 'block';\n",
       "            status.innerHTML = 'âœ… Recording saved! Click <strong>Process Audio</strong> below.';\n",
       "            status.style.color = '#55efc4';\n",
       "\n",
       "            // Store blob for later use\n",
       "            window.recordedAudioBlob = audioBlob;\n",
       "        };\n",
       "\n",
       "        mediaRecorder.start();\n",
       "        status.textContent = 'ğŸ™ï¸ Recording... Speak now!';\n",
       "        status.style.color = '#74b9ff';\n",
       "        recordBtn.disabled = true;\n",
       "        recordBtn.style.opacity = '0.5';\n",
       "        stopBtn.disabled = false;\n",
       "        stopBtn.style.opacity = '1';\n",
       "\n",
       "    } catch(err) {\n",
       "        let errorMsg = err.message;\n",
       "        let helpText = '';\n",
       "\n",
       "        if (err.name === 'NotAllowedError' || errorMsg.includes('Permission denied')) {\n",
       "            helpText = '<br>ğŸ‘‰ Use the <strong>File Upload</strong> option above instead (works over SSH).';\n",
       "        } else if (err.name === 'NotFoundError') {\n",
       "            helpText = '<br>ğŸ‘‰ No microphone detected. Use <strong>File Upload</strong> instead.';\n",
       "        }\n",
       "\n",
       "        status.innerHTML = 'âŒ ' + errorMsg + helpText;\n",
       "        status.style.color = '#fab1a0';\n",
       "    }\n",
       "};\n",
       "\n",
       "stopBtn.onclick = () => {\n",
       "    if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
       "        mediaRecorder.stop();\n",
       "        mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
       "        recordBtn.disabled = false;\n",
       "        recordBtn.style.opacity = '1';\n",
       "        stopBtn.disabled = true;\n",
       "        stopBtn.style.opacity = '0.5';\n",
       "    }\n",
       "};\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "â–¶ï¸  PROCESS AUDIO\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b6e404caac4c30a86bfe1c5712acd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Use sample audio (robotics.flac) for testing', layout=Layout(width='400px')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17767917194c4e0e85c5f54518c14f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='ğŸ¤ Process Audio', layout=Layout(height='50px', width='300px'), styâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a84b01758254971a7ac71202253e8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OPTION 1: File Upload (Works over SSH / Remote Jupyter)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“ OPTION 1: Upload Audio File (Recommended for SSH/Remote)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Upload a .wav, .mp3, .flac, .webm, or .ogg file from your local machine.\\n\")\n",
    "\n",
    "# File upload widget\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.wav,.mp3,.flac,.webm,.ogg,.m4a',\n",
    "    multiple=False,\n",
    "    description='Upload Audio',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "uploaded_audio_path = None\n",
    "\n",
    "def handle_upload(change):\n",
    "    \"\"\"Save uploaded file to disk.\"\"\"\n",
    "    global uploaded_audio_path\n",
    "    if file_upload.value:\n",
    "        # Handle both old dict format and new tuple format\n",
    "        uploaded_files = file_upload.value\n",
    "        \n",
    "        if isinstance(uploaded_files, dict):\n",
    "            # Old format: {'filename': {'content': bytes, ...}}\n",
    "            uploaded_file = list(uploaded_files.values())[0]\n",
    "            content = uploaded_file['content']\n",
    "            filename = list(uploaded_files.keys())[0]\n",
    "        else:\n",
    "            # New format: tuple of FileUpload.FileInfo objects\n",
    "            uploaded_file = uploaded_files[0]\n",
    "            content = uploaded_file.content\n",
    "            filename = uploaded_file.name\n",
    "        \n",
    "        # Save to temp file with original extension\n",
    "        ext = os.path.splitext(filename)[1] or '.wav'\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as f:\n",
    "            f.write(content)\n",
    "            uploaded_audio_path = f.name\n",
    "        \n",
    "        print(f\"âœ… Uploaded: {filename} â†’ saved to {uploaded_audio_path}\")\n",
    "\n",
    "file_upload.observe(handle_upload, names='value')\n",
    "display(file_upload)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OPTION 2: Browser Microphone (Local Jupyter Only)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ™ï¸ OPTION 2: Browser Microphone (Local Jupyter Only)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âš ï¸  This requires localhost or HTTPS. Will show 'Permission denied' over SSH.\\n\")\n",
    "\n",
    "# HTML5 Audio Recorder with better error handling\n",
    "html_recorder = HTML(\"\"\"\n",
    "<div style=\"padding: 20px; background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 12px; text-align: center; color: #eee; font-family: system-ui, -apple-system, sans-serif;\">\n",
    "    <h3 style=\"margin-top: 0; color: #00d4ff;\">ğŸ™ï¸ Browser Microphone</h3>\n",
    "    <p style=\"color: #aaa; font-size: 14px;\"><strong>Note:</strong> Only works on localhost or HTTPS connections</p>\n",
    "    <div id=\"controls\" style=\"margin: 20px 0;\">\n",
    "        <button id=\"recordBtn\" style=\"background: linear-gradient(135deg, #00b894 0%, #00cec9 100%); color: white; padding: 15px 30px; font-size: 16px; border: none; border-radius: 8px; margin: 5px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 15px rgba(0,184,148,0.3);\">\n",
    "            ğŸ™ï¸ Start Recording\n",
    "        </button>\n",
    "        <button id=\"stopBtn\" disabled style=\"background: linear-gradient(135deg, #e17055 0%, #d63031 100%); color: white; padding: 15px 30px; font-size: 16px; border: none; border-radius: 8px; margin: 5px; cursor: pointer; opacity: 0.5; font-weight: 600;\">\n",
    "            â¹ï¸ Stop Recording\n",
    "        </button>\n",
    "    </div>\n",
    "    <div id=\"status\" style=\"padding: 15px; font-weight: bold; color: #ffeaa7; background: rgba(255,255,255,0.1); border-radius: 8px; margin: 10px 0;\"></div>\n",
    "    <audio id=\"audioPlayback\" controls style=\"margin-top: 20px; display: none; width: 100%;\"></audio>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "let mediaRecorder;\n",
    "let audioChunks = [];\n",
    "\n",
    "const recordBtn = document.getElementById('recordBtn');\n",
    "const stopBtn = document.getElementById('stopBtn');\n",
    "const status = document.getElementById('status');\n",
    "const audioPlayback = document.getElementById('audioPlayback');\n",
    "\n",
    "// Check if we're on a secure context\n",
    "if (!window.isSecureContext) {\n",
    "    status.innerHTML = 'âš ï¸ <strong>Not a secure context.</strong> Microphone requires localhost or HTTPS.<br>ğŸ‘‰ Use the <strong>File Upload</strong> option above instead.';\n",
    "    status.style.color = '#fab1a0';\n",
    "    recordBtn.disabled = true;\n",
    "    recordBtn.style.opacity = '0.5';\n",
    "}\n",
    "\n",
    "recordBtn.onclick = async () => {\n",
    "    try {\n",
    "        status.textContent = 'ğŸ”„ Requesting microphone access...';\n",
    "        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "        mediaRecorder = new MediaRecorder(stream);\n",
    "        audioChunks = [];\n",
    "        \n",
    "        mediaRecorder.ondataavailable = (event) => {\n",
    "            audioChunks.push(event.data);\n",
    "        };\n",
    "        \n",
    "        mediaRecorder.onstop = () => {\n",
    "            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
    "            const audioUrl = URL.createObjectURL(audioBlob);\n",
    "            audioPlayback.src = audioUrl;\n",
    "            audioPlayback.style.display = 'block';\n",
    "            status.innerHTML = 'âœ… Recording saved! Click <strong>Process Audio</strong> below.';\n",
    "            status.style.color = '#55efc4';\n",
    "            \n",
    "            // Store blob for later use\n",
    "            window.recordedAudioBlob = audioBlob;\n",
    "        };\n",
    "        \n",
    "        mediaRecorder.start();\n",
    "        status.textContent = 'ğŸ™ï¸ Recording... Speak now!';\n",
    "        status.style.color = '#74b9ff';\n",
    "        recordBtn.disabled = true;\n",
    "        recordBtn.style.opacity = '0.5';\n",
    "        stopBtn.disabled = false;\n",
    "        stopBtn.style.opacity = '1';\n",
    "        \n",
    "    } catch(err) {\n",
    "        let errorMsg = err.message;\n",
    "        let helpText = '';\n",
    "        \n",
    "        if (err.name === 'NotAllowedError' || errorMsg.includes('Permission denied')) {\n",
    "            helpText = '<br>ğŸ‘‰ Use the <strong>File Upload</strong> option above instead (works over SSH).';\n",
    "        } else if (err.name === 'NotFoundError') {\n",
    "            helpText = '<br>ğŸ‘‰ No microphone detected. Use <strong>File Upload</strong> instead.';\n",
    "        }\n",
    "        \n",
    "        status.innerHTML = 'âŒ ' + errorMsg + helpText;\n",
    "        status.style.color = '#fab1a0';\n",
    "    }\n",
    "};\n",
    "\n",
    "stopBtn.onclick = () => {\n",
    "    if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
    "        mediaRecorder.stop();\n",
    "        mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
    "        recordBtn.disabled = false;\n",
    "        recordBtn.style.opacity = '1';\n",
    "        stopBtn.disabled = true;\n",
    "        stopBtn.style.opacity = '0.5';\n",
    "    }\n",
    "};\n",
    "</script>\n",
    "\"\"\")\n",
    "\n",
    "display(html_recorder)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Process Button (Works with either option)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â–¶ï¸  PROCESS AUDIO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description=\"ğŸ¤ Process Audio\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "# Option to use sample audio for testing\n",
    "use_sample_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Use sample audio (robotics.flac) for testing',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_process_click(b):\n",
    "    \"\"\"Process the uploaded or recorded audio.\"\"\"\n",
    "    global uploaded_audio_path\n",
    "    \n",
    "    with output_widget:\n",
    "        output_widget.clear_output()\n",
    "        \n",
    "        # Determine which audio file to use\n",
    "        if use_sample_checkbox.value:\n",
    "            audio_file = sample_audio_path\n",
    "            print(f\"ğŸ“‚ Using sample audio: {audio_file}\")\n",
    "        elif uploaded_audio_path and os.path.exists(uploaded_audio_path):\n",
    "            audio_file = uploaded_audio_path\n",
    "            print(f\"ğŸ“‚ Using uploaded audio: {audio_file}\")\n",
    "        else:\n",
    "            print(\"âŒ No audio file available!\")\n",
    "            print(\"   â†’ Upload a file using Option 1, OR\")\n",
    "            print(\"   â†’ Record using Option 2 (local only), OR\")\n",
    "            print(\"   â†’ Check 'Use sample audio' for testing\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nğŸ¤ Processing voice input...\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        result = voice_agent.run_voice(audio_file, verbose=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ“‹ FINAL RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"âŒ Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"ğŸ“ Transcription: {result['query']}\")\n",
    "            print(f\"\\nğŸ¤– Response:\\n{result['response']}\")\n",
    "            print(f\"\\nğŸ›¡ï¸ Safety: {'âœ… SAFE' if result['is_safe'] else 'âš ï¸ UNSAFE'}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "process_button.on_click(on_process_click)\n",
    "\n",
    "display(use_sample_checkbox)\n",
    "display(process_button)\n",
    "display(output_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've built a **voice-powered LangChain v1.0 agent** with RAG using NVIDIA Nemotron models:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     VOICE RAG AGENT STACK (LangChain v1.0)                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Component          Model                           Purpose                â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€                           â”€â”€â”€â”€â”€â”€â”€                â”‚\n",
    "â”‚   ğŸ¤ ASR             nemotron-speech-streaming       Voice â†’ Text           â”‚\n",
    "â”‚   ğŸ“š Embeddings      llama-nemotron-embed-vl         Semantic search        â”‚\n",
    "â”‚   ğŸ”„ Reranking       llama-nemotron-rerank-vl        Sharpen accuracy       â”‚\n",
    "â”‚   ğŸ–¼ï¸ Vision (VLM)    nemotron-nano-12b-vl            Describe images        â”‚ \n",
    "â”‚   ğŸ¤– Agent LLM       nemotron-3-nano-30b             Agent reasoning        â”‚\n",
    "â”‚   ğŸ›¡ï¸ Safety          Llama-3.1-Nemotron-Safety       Input/Output checks    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Architecture:      LangChain v1.0 Agent (CompiledStateGraph)              â”‚\n",
    "â”‚   API:               langchain.agents.create_agent                          â”‚\n",
    "â”‚   Model:             ChatNVIDIA (langchain_nvidia_ai_endpoints)             â”‚\n",
    "â”‚   Tools:             - search_knowledge_base (RAG + VLM on-demand)          â”‚\n",
    "â”‚   Input:             ğŸ™ï¸ Voice only (microphone)                             â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## The 5-Step Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 1: Voice Input                                          â”‚\n",
    "â”‚  ğŸ¤ Microphone â†’ Nemotron ASR â†’ Text Query                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 2: Input Safety Check                                   â”‚\n",
    "â”‚  ğŸ›¡ï¸ Nemotron Safety Guard validates input                     â”‚\n",
    "â”‚      â”œâ”€â”€ UNSAFE â†’ âŒ Reject immediately                       â”‚\n",
    "â”‚      â””â”€â”€ SAFE   â†’ âœ… Continue to agent                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 3: Agent Processing (create_agent loop)                 â”‚\n",
    "â”‚  ğŸ¤– Agent decides: Need more information?                     â”‚\n",
    "â”‚      â”œâ”€â”€ YES â†’ Call RAG Tool                                  â”‚\n",
    "â”‚      â”‚         â””â”€â”€ Retrieve + Rerank docs                     â”‚\n",
    "â”‚      â”‚         â””â”€â”€ If images â†’ VLM describes them             â”‚\n",
    "â”‚      â”‚         â””â”€â”€ Return context to agent                    â”‚\n",
    "â”‚      â”‚         â””â”€â”€ Loop back to decide again                  â”‚\n",
    "â”‚      â””â”€â”€ NO  â†’ Generate final response                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 4: Output Safety Check                                  â”‚\n",
    "â”‚  ğŸ›¡ï¸ Nemotron Safety Guard validates response                  â”‚\n",
    "â”‚      â”œâ”€â”€ UNSAFE â†’ âŒ Filter/block response                    â”‚\n",
    "â”‚      â””â”€â”€ SAFE   â†’ âœ… Continue to output                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 5: Return Safe Response                                 â”‚\n",
    "â”‚  âœ… Deliver grounded, safe response to user                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## LangChain v1.0 Implementation Details\n",
    "\n",
    "**Modern create_agent API:**\n",
    "- âœ… Uses `langchain.agents.create_agent` (LangChain v1.0)\n",
    "- âœ… Returns `CompiledStateGraph` that auto-loops\n",
    "- âœ… Integrates with `ChatNVIDIA` for NVIDIA endpoints\n",
    "- âœ… Simple message format: `{\"messages\": [(\"user\", query)]}`\n",
    "\n",
    "**Voice-First Design:**\n",
    "- ğŸ¤ HTML5 MediaRecorder for browser microphone access\n",
    "- ğŸ™ï¸ No webcam - audio only\n",
    "- ğŸ“ Automatic transcription with Nemotron ASR\n",
    "\n",
    "**Safety & Quality:**\n",
    "- ğŸ›¡ï¸ Input safety check (before agent runs)\n",
    "- ğŸ›¡ï¸ Output safety check (after agent completes)\n",
    "- ğŸ–¼ï¸ VLM processes images when retrieved\n",
    "- ğŸ” Multimodal RAG (text + images)\n",
    "- ğŸ¯ Reranking for accuracy (+6-7%)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Add more tools**: Web search, calculators, code execution\n",
    "- **Enable streaming**: Use `agent.stream()` for real-time responses\n",
    "- **Add memory**: Conversation history across multiple queries\n",
    "- **Deploy**: Use NVIDIA NIM for production inference\n",
    "- **Custom data**: Index your own documents and images\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [LangChain create_agent Docs](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "- [ChatNVIDIA Integration](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/)\n",
    "- [Nemotron Models](https://huggingface.co/nvidia)\n",
    "- [NVIDIA NIM](https://developer.nvidia.com/nim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}