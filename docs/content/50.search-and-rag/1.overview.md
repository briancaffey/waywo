---
title: Overview
description: Three retrieval modes for exploring the project index -- semantic search, agentic RAG chat, and voice chat.
navigation.icon: i-lucide-search
---

# Search & RAG Overview

WAYWO provides multiple ways to query the project index. All modes share the same vector embeddings and retrieval pipeline but serve different use cases.

## Semantic Search

A direct path from query text to ranked results.

1. User submits a natural-language query
2. Query is embedded into a 4096-dimensional vector via the NVIDIA embedding service
3. sqlite-vector performs cosine similarity against all project embeddings
4. Top-k results are returned with similarity scores (optionally reranked)

**Endpoint:** `POST /api/semantic-search`

```json
{
  "query": "real-time collaboration tools",
  "limit": 10,
  "use_rerank": true
}
```

Semantic search is fast and transparent -- you see the raw similarity scores and can page through results yourself.

## Agentic RAG Chat

An LLM agent that autonomously decides when to search the project database. The agent uses **OpenAI-compatible tool calling** to invoke RAG tools, then synthesizes a conversational answer grounded in real project data.

1. User sends a message via the text chat or voice chat interface
2. The agent receives the message along with tool definitions (`search_projects`, `get_project_details`)
3. For factual questions, the model calls `search_projects` -- which runs the full embed → vector search → rerank pipeline
4. The model may call tools multiple times (e.g., search then get details for a specific project)
5. Once satisfied, the model generates a natural-language answer citing the retrieved projects
6. The answer is streamed token-by-token to the client

**Endpoint:** `POST /api/chat/threads/{id}/message/stream` (SSE)
**Endpoint:** `/ws/voice-chat` (WebSocket)

The agent emits structured events (`thinking`, `tool_call`, `tool_result`, `answer_token`, `answer_done`) that the frontend renders as a step-by-step reasoning trace alongside the final answer.

### Two Operating Modes

| Mode | Setting | How tools are called |
|------|---------|---------------------|
| **Tool calling** (default) | `AGENT_TOOL_CALLING=true` | LLM uses structured tool-calling API via vLLM |
| **Proactive RAG** (fallback) | `AGENT_TOOL_CALLING=false` | Engine proactively calls RAG for factual queries |

## Legacy RAG Chatbot

The original single-shot RAG endpoint is still available for backward compatibility:

**Endpoint:** `POST /api/waywo-chatbot`

This endpoint runs a LlamaIndex `Workflow` that always retrieves context before generating a response. It does not support multi-turn conversation, streaming, or agent-style tool use. New integrations should use the agentic chat endpoint instead.

## When to Use Each

| Scenario | Recommended Mode |
|----------|-----------------|
| Browsing and exploring projects by topic | Semantic Search |
| Filtering results with your own criteria | Semantic Search |
| Conversational Q&A ("Are there any Rust CLI tools?") | Agentic RAG Chat |
| Voice-based project exploration | Agentic RAG Chat (voice) |
| Getting a summarized overview of a topic area | Agentic RAG Chat |
| Programmatic integrations needing structured result lists | Semantic Search |

All modes depend on the same prerequisites: a running embedding service, projects with stored embeddings, and (for best relevance) a running rerank service.
