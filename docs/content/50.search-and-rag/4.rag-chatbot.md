---
title: Agentic RAG Chat
description: How the agentic chat system uses OpenAI-compatible tool calling to ground LLM answers in the project database.
navigation.icon: i-lucide-message-square
---

# Agentic RAG Chat

The chat system uses an **agentic architecture** where the LLM decides when to call RAG tools via structured tool calling. Instead of always retrieving context for every message, the agent autonomously determines whether a query needs database lookup (factual questions about projects) or can be answered directly (greetings, follow-ups).

## Architecture Overview

```
User message
  |
  v
run_agent()  ──────────────────────────────────────────────────┐
  |                                                            |
  |  AGENT_TOOL_CALLING=true             AGENT_TOOL_CALLING=false
  |  (primary path)                      (fallback path)
  v                                      v
OpenAI chat.completions.create()     Heuristic classifier
  with tools=[search_projects,         |
              get_project_details]      |── factual → proactive search_projects
  |                                     |── conversational → direct LLM call
  |── tool_calls → execute tools,       |
  |   append results, loop back         v
  |── no tool_calls → final answer   LLM with injected context
  |                                      |
  v                                      v
AgentEvent stream                    AgentEvent stream
  (thinking, tool_call,                (thinking, tool_call,
   tool_result, answer_token,           tool_result, answer_token,
   answer_done)                         answer_done)
```

Both paths yield identical `AgentEvent` types, so the routes and frontend require zero changes regardless of which mode is active.

## Tool-Calling Path (Primary)

When `AGENT_TOOL_CALLING=true` (the default), the agent uses the **OpenAI-compatible tool-calling API** served by vLLM. This is the recommended mode.

### How It Works

1. **Build messages** -- System prompt + conversation history + user message in OpenAI format
2. **Call the LLM** -- `client.chat.completions.create(tools=TOOL_SCHEMAS, tool_choice="auto")`
3. **If response has `tool_calls`** -- Execute each tool, append `role: "tool"` results to the message history, loop back to step 2
4. **If response has content only** -- Treat as the final answer and stream it to the client
5. **Repeat** up to `AGENT_MAX_ITERATIONS` (default: 5)

### Available Tools

The agent has two tools, defined as OpenAI function-calling JSON schemas in `src/agent/tools.py`:

| Tool | Description | Parameters |
|------|-------------|------------|
| `search_projects` | Semantic search over the project database | `query` (string, required), `top_k` (int, default 5) |
| `get_project_details` | Get full details for a project by ID | `project_id` (int, required) |

Both tools are thin wrappers around existing functions:

- `search_projects` calls `smart_retrieve()` from `src/rag/retrieve.py` (embed → vector search → rerank)
- `get_project_details` calls `get_project()` from `src/db/projects.py`

### Hermes Tool-Call Parsing

The Nemotron model uses the **Hermes tool-call format** (`<tool_call>` XML tags). vLLM's `--tool-call-parser hermes` flag converts these into structured `tool_calls` on the API response. Occasionally the parser misses a call and the raw `<tool_call>` tags appear in the content field. The engine detects this and executes them as real tool calls:

```python
# From src/agent/engine.py
remaining_text, hermes_calls = _extract_hermes_tool_calls(content_text)
if hermes_calls:
    # Execute each parsed tool call and loop back
    ...
```

### vLLM Server Requirements

The vLLM server must be started with tool-calling enabled:

```bash
vllm serve nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes
```

Without these flags, the model cannot produce structured tool calls. Use `AGENT_TOOL_CALLING=false` to fall back to proactive RAG.

## Proactive RAG Path (Fallback)

When `AGENT_TOOL_CALLING=false`, the engine bypasses LLM tool calling entirely and uses a simpler heuristic approach.

### How It Works

1. **Classify the query** -- A keyword-based heuristic checks if the message mentions projects, technologies, or search-related terms
2. **Factual queries** -- Proactively call `search_projects` with the user's message, inject the results as a system message, then call the LLM
3. **Conversational queries** -- Skip RAG and call the LLM directly

This path is useful when:
- vLLM is not configured with `--enable-auto-tool-choice`
- You want deterministic RAG behavior without relying on the model's tool-use decisions
- The model doesn't reliably use tool calling

## The Retrieval Pipeline

Both agent paths call the same underlying retrieval function when searching for projects. The `smart_retrieve()` function in `src/rag/retrieve.py` implements the full RAG pipeline:

```
Query text
  |
  v
Embed query (NVIDIA embedding service, 4096 dims)
  |
  v
Semantic search (sqlite-vector cosine similarity, top_k * 3 candidates)
  |
  v
Similarity threshold gate (RAG_SIMILARITY_THRESHOLD)
  |
  v
Rerank (llama-nemotron-rerank-1b cross-encoder, keep top_k)
  |
  v
Build context text + source project metadata
```

The pipeline includes several resilience features:
- **Embedding failure** -- Returns empty context (agent responds without RAG)
- **Rerank failure** -- Falls back to similarity-ordered results
- **Below threshold** -- Returns empty context to avoid low-quality retrieval

## Agent Events

The agent yields `AgentEvent` objects as it works. Both the SSE streaming endpoint (`/api/chat/threads/{id}/message/stream`) and the WebSocket voice endpoint (`/ws/voice-chat`) consume these events identically.

| Event Type | When | Data |
|------------|------|------|
| `thinking` | Model produced reasoning text | `thought`, `iteration` |
| `tool_call` | About to execute a tool | `tool`, `input`, `iteration` |
| `tool_result` | Tool execution completed | `tool`, `projects_found`, `result_summary` |
| `answer_start` | Final answer streaming begins | -- |
| `answer_token` | One token of the final answer | `token` |
| `answer_done` | Complete answer with sources | `full_text`, `source_projects`, `agent_steps` |
| `error` | Something went wrong | `error` |
| `max_iterations` | Hit iteration limit | `iterations` |

## System Prompts

The system prompts are defined in `src/agent/prompts.py`. There are two variants:

- **TEXT_AGENT_SYSTEM_PROMPT** -- Used by the text chat endpoint. Allows markdown formatting.
- **VOICE_AGENT_SYSTEM_PROMPT** -- Used by the voice chat endpoint. Instructs concise, spoken-natural responses without markdown.

Both prompts share the same core rules:
1. Use `search_projects` for any factual question about projects
2. Use `get_project_details` for specific project lookups
3. Never hallucinate project data
4. Respond directly (no tools) for greetings and general chat

Tools are not described in the prompt text -- they are passed via the `tools` parameter on the API call, where the model sees their JSON schemas directly.

## Configuration

| Setting | Env Var | Default | Description |
|---------|---------|---------|-------------|
| Tool calling mode | `AGENT_TOOL_CALLING` | `true` | Use OpenAI tool calling (`true`) or proactive RAG fallback (`false`) |
| Max iterations | `AGENT_MAX_ITERATIONS` | `5` | Maximum tool-use loops before forcing an answer |
| Similarity threshold | `RAG_SIMILARITY_THRESHOLD` | `0.65` | Minimum similarity for RAG to trigger in `smart_retrieve` |
| LLM temperature | `LLM_TEMPERATURE` | `0.7` | Sampling temperature for the agent LLM |
| LLM max tokens | `LLM_MAX_TOKENS` | `2048` | Maximum tokens per LLM response |

## File Map

| File | Role |
|------|------|
| `src/agent/engine.py` | Core agent loop (tool calling + fallback paths) |
| `src/agent/tools.py` | Tool definitions, executors, and OpenAI JSON schemas |
| `src/agent/prompts.py` | System prompts for text and voice modes |
| `src/agent/events.py` | `AgentEvent` and `AgentEventType` dataclasses |
| `src/rag/retrieve.py` | `smart_retrieve()` -- embed, search, rerank pipeline |
| `src/llm_config.py` | `get_openai_client()` factory for the AsyncOpenAI client |
| `src/routes/chat.py` | SSE streaming endpoint consuming `run_agent()` |
| `src/routes/voice.py` | WebSocket endpoint consuming `run_agent()` |
