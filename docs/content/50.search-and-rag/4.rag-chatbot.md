---
title: RAG Chatbot
description: The LlamaIndex workflow that powers the conversational chatbot -- from query embedding through retrieval, reranking, and LLM response generation.
navigation.icon: i-lucide-message-square
---

# RAG Chatbot

The RAG chatbot turns the project index into a conversational interface. Ask a question in plain English, and it returns a grounded answer that cites specific projects. The implementation is a LlamaIndex `Workflow` defined in `src/workflows/waywo_chatbot_workflow.py`.

## Workflow Pipeline

```
StartEvent
  |
  v
ChatQueryEvent          -- extract query and top_k
  |
  v
QueryEmbeddingEvent     -- embed the query (4096 dims)
  |
  v
ProjectsCandidatesEvent -- retrieve top_k * 3 candidates via vector search
  |
  v
ProjectsRetrievedEvent  -- rerank to top_k, build context string
  |
  v
StopEvent(ChatbotResult) -- LLM generates response with source attribution
```

Each arrow is a `@step`-decorated async method on `WaywoChatbotWorkflow`.

## Step-by-Step

### 1. Start -- ChatQueryEvent

Extracts the `query` string and `top_k` parameter from the incoming `StartEvent` and stores them in the workflow context.

### 2. Generate Query Embedding -- QueryEmbeddingEvent

Calls the NVIDIA embedding service to convert the user's query into a 4096-dimensional vector:

```python
query_embedding = await get_single_embedding(
    text=ev.query,
    embedding_url=self.embedding_url,
)
```

If the embedding service is unreachable, an empty embedding is returned, which results in zero candidates downstream.

### 3. Retrieve Candidates -- ProjectsCandidatesEvent

Performs a semantic search for `top_k * candidate_multiplier` projects (default multiplier is 3, so `top_k=5` retrieves 15 candidates):

```python
candidate_limit = ev.top_k * self.candidate_multiplier
results = semantic_search(
    query_embedding=ev.query_embedding,
    limit=candidate_limit,
    is_valid=True,
)
```

Over-fetching gives the reranker a larger pool to select from, improving final relevance.

### 4. Rerank Projects -- ProjectsRetrievedEvent

Each candidate's `title: description` is scored by the llama-nemotron-rerank-1b cross-encoder model. The top `top_k` candidates (by rerank score) are kept:

```python
documents = [f"{c['title']}: {c['description']}" for c in ev.candidates]

rerank_result = await rerank_documents(
    query=ev.query,
    documents=documents,
    rerank_url=self.rerank_url,
)
```

A formatted context string is assembled with project titles, descriptions, tags, and scores for the LLM prompt.

**Fallback:** If the rerank service is unavailable (`RerankError`), the workflow falls back to the original similarity order and logs a warning. Search quality degrades but the chatbot remains functional.

### 5. Generate Response -- StopEvent

The Nemotron LLM receives a prompt combining the system instructions, the project context, and the user's question:

```python
prompt = chatbot_response_prompt(query=ev.query, context=ev.context)
response = await self.llm.acomplete(prompt)
```

The system prompt (defined in `src/workflows/prompts.py`) instructs the model to:

- Base answers on the provided project context
- Mention specific projects by name
- Include relevant details like tags and scores
- Acknowledge when no relevant projects are found

## ChatbotResult

The workflow returns a `ChatbotResult` dataclass:

| Field | Type | Description |
|-------|------|-------------|
| `response` | `str` | The generated natural-language answer |
| `source_projects` | `list[dict]` | Projects used as context (id, title, scores, tags) |
| `query` | `str` | The original user query |
| `projects_found` | `int` | Number of source projects |

The API response at `POST /api/waywo-chatbot` mirrors this structure directly:

```json
{
  "response": "There are several AI-related projects...",
  "source_projects": [
    {
      "id": 42,
      "title": "NeuroSketch",
      "short_description": "AI-powered drawing assistant",
      "rerank_score": 0.9451,
      "similarity": 0.8723,
      "hashtags": ["ai", "creative", "drawing"],
      "idea_score": 8,
      "complexity_score": 7
    }
  ],
  "query": "What AI projects are people building?",
  "projects_found": 5
}
```

## Convenience Function

For one-off queries outside the workflow lifecycle, use `run_chatbot_query()`:

```python
from src.workflows.waywo_chatbot_workflow import run_chatbot_query

result = await run_chatbot_query(
    query="What Rust CLI tools are people building?",
    top_k=5,
)
print(result.response)
```

This creates a fresh `WaywoChatbotWorkflow` instance, runs the query, and returns the result with a 120-second timeout.
