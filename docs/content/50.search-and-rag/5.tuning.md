---
title: Tuning
description: Practical guidance on adjusting agent behavior, retrieval parameters, prompt configuration, and the tool-calling vs fallback mode.
navigation.icon: i-lucide-sliders-horizontal
---

# Tuning

This page covers the levers you can pull to improve search relevance, chatbot quality, and overall performance.

## Agent Mode

The `AGENT_TOOL_CALLING` environment variable controls how the agent invokes RAG:

| Value | Behavior |
|-------|----------|
| `true` (default) | LLM decides when to call tools via the OpenAI tool-calling API. Requires vLLM with `--enable-auto-tool-choice --tool-call-parser hermes`. |
| `false` | Engine proactively calls `search_projects` for factual queries using a keyword heuristic. No tool-calling support needed from the model server. |

**When to use fallback mode:**
- vLLM is not configured with tool-calling flags
- The model frequently fails to produce valid tool calls
- You want deterministic RAG behavior for every factual query

## Max Iterations

`AGENT_MAX_ITERATIONS` (default: 5) caps how many LLM round-trips the agent can make. Each iteration may include one or more tool calls. Typical flows use 2 iterations (one to call `search_projects`, one to generate the answer). Increase this if you want the agent to do multi-step lookups (e.g., search then get details for several projects).

## top_k

Controls how many projects are used as context for the chatbot, or returned as results for search.

| Value | Tradeoff |
|-------|----------|
| **1--3** | Fast, focused answers. May miss relevant projects if the best match is not in the top few. |
| **5** (default) | Good balance of coverage and speed. Works well for most queries. |
| **10--20** | Broader context for complex queries. Slower response times and more LLM tokens consumed. |

For the RAG pipeline, `top_k` interacts with the `candidate_multiplier` (default 3). Setting `top_k=5` means 15 candidates are retrieved from vector search, then the reranker picks the best 5. Increasing `top_k` proportionally increases the candidate pool.

The semantic search endpoint (`POST /api/semantic-search`) uses the `limit` parameter instead, which directly controls the final result count.

## Similarity Threshold

`RAG_SIMILARITY_THRESHOLD` (default: 0.65) is the minimum cosine similarity for RAG to activate in `smart_retrieve()`. If the best vector match scores below this threshold, the function returns empty context and the agent responds without project data.

- **Lower values** (e.g., 0.5) -- More queries trigger RAG, but with potentially less relevant results
- **Higher values** (e.g., 0.8) -- Only very relevant matches trigger RAG, reducing noise

::note
When using the tool-calling agent path, `search_projects` calls `smart_retrieve()` with `similarity_threshold=0.0` to always return results and let the model judge relevance. The threshold primarily affects the legacy RAG endpoint and the proactive fallback path.
::

## Reranker

The reranker (llama-nemotron-rerank-1b) is a cross-encoder model that scores each candidate against the query. It significantly improves relevance compared to raw vector similarity alone.

**Why it helps:** Embedding similarity is computed independently for the query and each document. The reranker sees both together, allowing it to capture fine-grained relevance signals that bi-encoder embeddings miss.

**Performance impact:** Reranking adds one network round-trip to the rerank service. Latency scales linearly with the number of candidates. For 15 candidates (the default for `top_k=5`), this typically adds 200--500ms.

**Disabling reranking:** Set `use_rerank: false` in the semantic search request body. The agent tools always attempt reranking but fall back to similarity order if the service is unavailable.

**Health check:** `GET /api/rerank/health` confirms the service is reachable.

## Embedding Coverage

Projects without embeddings are invisible to both search and the chatbot. Check your coverage:

```
GET /api/semantic-search/stats
```

```json
{
  "total_projects": 1250,
  "projects_with_embeddings": 1180,
  "embedding_coverage": 94.4
}
```

Common reasons for missing embeddings:

- The embedding service was down when the project was processed
- The project was imported before the embedding pipeline was added
- The embedding request timed out for very long descriptions

To fix gaps, reprocess the affected comments via the API or re-run the embedding step. After adding new embeddings, rebuild the quantization index:

```
POST /api/admin/rebuild-vector-index
```

**Embedding service health:** `GET /api/embedding/health` confirms the service is reachable.

## Prompt Tuning

The agent system prompts are defined in `src/agent/prompts.py`. There are two variants:

- **`TEXT_AGENT_SYSTEM_PROMPT`** -- Used by text chat. Allows markdown formatting.
- **`VOICE_AGENT_SYSTEM_PROMPT`** -- Used by voice chat. Instructs concise, natural speech without markdown.

Both prompts share core rules:

```
1. Use search_projects for ANY question about projects, topics, or technologies
2. Use get_project_details for specific project lookups by ID
3. NEVER hallucinate project data -- only reference tool results
4. Respond directly (no tools) for greetings and general chat
```

Adjustments you might make:

- **Response length** -- Add instructions like "Keep responses under 3 paragraphs"
- **Formatting** -- Request bullet points, numbered lists, or markdown tables
- **Attribution style** -- Control how projects are cited (inline mentions vs. a references section)
- **Tool-use aggressiveness** -- Strengthen or soften the "always search first" instruction

Note that tool definitions are passed via the API `tools` parameter, not embedded in the prompt text. The model sees the JSON schemas directly.

## LLM Temperature

`LLM_TEMPERATURE` (default: 0.7) controls the randomness of the model's output. Lower values (0.1--0.3) produce more focused, deterministic answers. Higher values (0.8--1.0) increase variety but may reduce accuracy.

For the agent, temperature also affects tool-calling decisions. A very high temperature may cause the model to skip tool calls or call irrelevant tools.

## Services Health Dashboard

The admin endpoint gives a combined view of all AI service dependencies:

```
GET /api/admin/services-health
```

This checks the LLM, embedding, and rerank services in one call and returns their status, URLs, and available models. Use this to quickly diagnose why search or chat results are degraded.
