---
title: Tuning
description: Practical guidance on adjusting top_k, reranking, embedding coverage, and prompt configuration for better search and chatbot results.
navigation.icon: i-lucide-sliders-horizontal
---

# Tuning

This page covers the levers you can pull to improve search relevance, chatbot quality, and overall performance.

## top_k

Controls how many projects are used as context for the chatbot, or returned as results for search.

| Value | Tradeoff |
|-------|----------|
| **1--3** | Fast, focused answers. May miss relevant projects if the best match is not in the top few. |
| **5** (default) | Good balance of coverage and speed. Works well for most queries. |
| **10--20** | Broader context for complex queries. Slower response times and more LLM tokens consumed. |

For the RAG chatbot, remember that `top_k` interacts with the `candidate_multiplier` (default 3). Setting `top_k=5` means 15 candidates are retrieved from vector search, then the reranker picks the best 5. Increasing `top_k` proportionally increases the candidate pool.

The semantic search endpoint (`POST /api/semantic-search`) uses the `limit` parameter instead, which directly controls the final result count.

## Reranker

The reranker (llama-nemotron-rerank-1b) is a cross-encoder model that scores each candidate against the query. It significantly improves relevance compared to raw vector similarity alone.

**Why it helps:** Embedding similarity is computed independently for the query and each document. The reranker sees both together, allowing it to capture fine-grained relevance signals that bi-encoder embeddings miss.

**Performance impact:** Reranking adds one network round-trip to the rerank service. Latency scales linearly with the number of candidates. For 15 candidates (the default for `top_k=5`), this typically adds 200--500ms.

**Disabling reranking:** Set `use_rerank: false` in the semantic search request body. The chatbot workflow always attempts reranking but falls back to similarity order if the service is unavailable.

**Health check:** `GET /api/rerank/health` confirms the service is reachable.

## Embedding Coverage

Projects without embeddings are invisible to both search and the chatbot. Check your coverage:

```
GET /api/semantic-search/stats
```

```json
{
  "total_projects": 1250,
  "projects_with_embeddings": 1180,
  "embedding_coverage": 94.4
}
```

Common reasons for missing embeddings:

- The embedding service was down when the project was processed
- The project was imported before the embedding pipeline was added
- The embedding request timed out for very long descriptions

To fix gaps, reprocess the affected comments via the API or re-run the embedding step. After adding new embeddings, rebuild the quantization index:

```
POST /api/admin/rebuild-vector-index
```

**Embedding service health:** `GET /api/embedding/health` confirms the service is reachable.

## Prompt Tuning

The chatbot's response style is controlled by the system prompt in `src/workflows/prompts.py`:

```python
CHATBOT_SYSTEM_PROMPT = """You are an AI assistant that helps users
explore projects from Hacker News "What are you working on?" threads.

You have access to a database of projects that people have shared.
When answering questions:

1. Base your answers on the provided project context
2. If relevant projects are found, mention specific projects by name
3. Include relevant details like descriptions, tags, and scores when helpful
4. If no relevant projects are found, say so honestly
5. Be helpful and conversational
...
"""
```

Adjustments you might make:

- **Response length** -- Add instructions like "Keep responses under 3 paragraphs" or "Be concise"
- **Formatting** -- Request bullet points, numbered lists, or markdown tables
- **Personality** -- Adjust tone from neutral to enthusiastic, technical, or casual
- **Attribution style** -- Control how projects are cited (inline mentions vs. a references section)

The response prompt template (`CHATBOT_RESPONSE_TEMPLATE`) assembles the system prompt, the retrieved project context, and the user query into a single LLM call. You can inspect all active prompts at runtime via:

```
GET /api/workflow-prompts
```

## Services Health Dashboard

The admin endpoint gives a combined view of all AI service dependencies:

```
GET /api/admin/services-health
```

This checks the LLM, embedding, and rerank services in one call and returns their status, URLs, and available models. Use this to quickly diagnose why search or chat results are degraded.
