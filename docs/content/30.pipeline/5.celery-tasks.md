---
title: Celery Tasks
description: Task definitions, queue configuration, scheduling, and how to trigger pipeline processing manually or on a schedule.
navigation.icon: i-lucide-list-todo
---

# Celery Tasks

All pipeline processing runs as Celery tasks on the `waywo` queue. Tasks are defined in `src/tasks.py`, the Celery app is configured in `src/celery_app.py`, and the beat schedule lives in `src/celery_beat.py`.

## Task Definitions

### `process_waywo_posts`

Batch-processes all posts from `waywo.yml`. Loads every entry and dispatches a `process_waywo_post` task for each one.

| Parameter        | Type       | Default | Description                          |
|------------------|------------|---------|--------------------------------------|
| `limit_posts`    | int | None | None    | Max posts to process (for testing)   |
| `limit_comments` | int | None | None    | Max comments per post (passed through) |

**Returns:** `{ "status": "queued", "posts_queued": N, "task_ids": [...] }`

### `process_waywo_post`

Fetches a single HN post and its top-level comments. Saves the post and comments to SQLite but does not run the project extraction workflow.

| Parameter        | Type       | Default | Description                        |
|------------------|------------|---------|------------------------------------|
| `post_id`        | int        | --      | HN item ID for the post            |
| `year`           | int | None | None    | Year metadata from `waywo.yml`     |
| `month`          | int | None | None    | Month metadata from `waywo.yml`    |
| `limit_comments` | int | None | None    | Max comments to fetch              |

**Returns:** `{ "status": "success", "post_id": N, "comments_saved": N, "comments_skipped": N }`

### `process_waywo_comment`

Processes a single comment through the full `WaywoProjectWorkflow`. This is the core task that performs AI extraction, validation, scoring, and embedding.

| Parameter    | Type | Default | Description            |
|--------------|------|---------|------------------------|
| `comment_id` | int  | --      | HN comment ID to process |

**Behavior:**

1. Fetches the comment from SQLite
2. Deletes any existing projects for this comment (enables reprocessing)
3. Runs the async LlamaIndex workflow
4. Saves valid projects to the database (with embeddings)
5. Marks the comment as processed

**Retries:** Up to 3 retries with exponential backoff (`countdown = 2^retries`).

**Returns:** `{ "status": "success", "comment_id": N, "valid_projects": N, "invalid_skipped": N }`

### `process_waywo_comments`

Batch-processes multiple comments by dispatching individual `process_waywo_comment` tasks.

| Parameter     | Type            | Default | Description                              |
|---------------|-----------------|---------|------------------------------------------|
| `limit`       | int | None      | None    | Max comments to process                  |
| `comment_ids` | list[int] | None | None    | Specific IDs to process (ignores `is_processed` flag) |

When `comment_ids` is not provided, it fetches all unprocessed comments from the database. When provided, it processes those specific comments regardless of their processing status.

**Returns:** `{ "status": "queued", "comments_queued": N, "task_ids": [...] }`

### `generate_ideas`

Generates synthetic project ideas using the NeMo Data Designer pipeline. Builds a multi-column NDD pipeline, runs LLM generation, and saves results with embeddings.

| Parameter    | Type              | Default | Description                              |
|--------------|-------------------|---------|------------------------------------------|
| `num_ideas`  | int               | 5       | Number of ideas to generate (1--50)      |
| `seed_tags`  | list[str] \| None | None    | Tags to guide generation                 |
| `creativity` | float             | 0.85    | LLM temperature for creative generation  |

**Behavior:**

1. Loads existing projects and computes tag co-occurrence statistics
2. Configures NDD provider, models, and pipeline
3. Runs `DataDesigner.create()` to generate records via LLM
4. For each row: parses fields, generates embedding, saves to DB with `source="nemo_data_designer"`
5. Reports progress via Celery state updates (`building_pipeline` → `generating` → `saving`)

**Returns:** `{ "status": "success|partial", "num_saved": N, "project_ids": [...], "errors": [...] }`

::tip
See [Data Designer](/data-designer/overview) for the full pipeline architecture and column definitions.
::

## Queue Configuration

All tasks run on the `waywo` queue. The Celery app is configured in `src/celery_app.py`:

```python
celery_app = Celery(
    "waywo",
    broker=os.getenv("CELERY_BROKER_URL", "redis://localhost:6379/0"),
    backend=os.getenv("CELERY_RESULT_BACKEND", "redis://localhost:6379/0"),
)

celery_app.conf.update(
    task_serializer="json",
    task_default_queue="waywo",
    task_routes={
        "process_waywo_posts": {"queue": "waywo"},
        "process_waywo_post": {"queue": "waywo"},
    },
)
```

The worker runs with **concurrency of 1** to avoid overwhelming external services:

```
celery -A src.celery_app worker --loglevel=info --queues=waywo --concurrency=1
```

## Triggering Tasks

### Via API

The FastAPI backend exposes endpoints that queue tasks and return the Celery task ID:

```bash
# Process all posts from waywo.yml
curl -X POST http://localhost:8000/api/process-waywo-posts \
  -H "Content-Type: application/json" \
  -d '{"limit_posts": 2, "limit_comments": 10}'

# Process all unprocessed comments
curl -X POST http://localhost:8000/api/process-waywo-comments \
  -H "Content-Type: application/json" \
  -d '{"limit": 50}'

# Process a single comment
curl -X POST http://localhost:8000/api/waywo-comments/12345678/process
```

### Via Flower

Flower (the Celery monitoring UI, typically at `http://localhost:5555`) provides a web interface to:

- View active, queued, and completed tasks
- Trigger tasks manually from the task type list
- Inspect task results and tracebacks
- Monitor worker status and resource usage

### Via Python / Shell

From within a Python shell or script with access to the Celery app:

```python
from src.tasks import process_waywo_posts, process_waywo_comment

# Queue all posts
process_waywo_posts.delay(limit_posts=5)

# Queue a single comment
process_waywo_comment.delay(comment_id=12345678)
```

## Beat Scheduler

Celery Beat is configured in `src/celery_beat.py` for automated, scheduled task execution. The schedule is currently a placeholder ready for custom cron entries:

```python
# src/celery_beat.py
beat_schedule = {}
```

To add a scheduled run (for example, processing new posts on the first of each month):

```python
from celery.schedules import crontab

beat_schedule = {
    "monthly-post-collection": {
        "task": "process_waywo_posts",
        "schedule": crontab(day_of_month="1", hour="6", minute="0"),
    },
    "daily-comment-processing": {
        "task": "process_waywo_comments",
        "schedule": crontab(hour="2", minute="0"),
    },
}
```

The beat schedule file is stored at `/app/celery-data/celerybeat-schedule` inside the Docker container.

## Worker Initialization

When a Celery worker process starts, it initializes OpenTelemetry tracing via the `worker_process_init` signal:

```python
@worker_process_init.connect
def init_worker_tracing(**kwargs):
    from src.tracing import init_tracing
    init_tracing(service_name="waywo-worker")
```

This enables LLM call tracing through Phoenix (Arize) for observability into the workflow execution within worker processes.
