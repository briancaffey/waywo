---
title: Nemotron LLM
description: Configuration and usage of the NVIDIA Nemotron-3-Nano-30B reasoning model for project extraction, validation, scoring, and chat.
navigation.icon: i-lucide-message-square
---

# Nemotron LLM

The `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4` model is the primary reasoning engine in WAYWO. It handles all natural language understanding tasks across the project processing pipeline and the RAG chatbot.

## Access Pattern

WAYWO connects to the Nemotron model through an **OpenAI-compatible API** using LlamaIndex's `OpenAILike` client. This means the model is served behind any OpenAI-compatible inference server (vLLM, TGI, etc.) and accessed via standard `/v1/chat/completions` endpoints.

```python
from llama_index.llms.openai_like import OpenAILike

llm = OpenAILike(
    api_base=LLM_BASE_URL,
    api_key=LLM_API_KEY,
    model=LLM_MODEL_NAME,
    temperature=0.7,
    max_tokens=2048,
    is_chat_model=True,
    is_function_calling_model=False,
)
```

**No function calling support.** The Nemotron model does not support OpenAI-style function/tool calling. Instead, WAYWO uses prompt engineering to instruct the model to return structured JSON in every response. Each prompt ends with an explicit instruction like `"Return ONLY valid JSON matching this structure"`.

## Two Temperature Modes

WAYWO provides two LLM factory functions with different temperature settings:

| Mode | Temperature | Used For |
|------|------------|----------|
| `get_llm()` | 0.7 | Creative tasks -- chatbot responses, project extraction |
| `get_llm_for_structured_output()` | 0.1 | Deterministic tasks -- validation, metadata, scoring |

The higher temperature (0.7) allows the model to produce more varied and natural responses in conversational contexts. The lower temperature (0.1) keeps output tightly focused on the requested JSON schema, reducing hallucination and format errors.

Both modes share the same max token limit of **2048 tokens**.

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_BASE_URL` | `http://192.168.6.19:8002/v1` | OpenAI-compatible API endpoint |
| `LLM_MODEL_NAME` | `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4` | Model identifier |
| `LLM_API_KEY` | `not-needed` | API key (not required for self-hosted) |
| `LLM_TEMPERATURE` | `0.7` | Default temperature for `get_llm()` |
| `LLM_MAX_TOKENS` | `2048` | Maximum output tokens |

## Pipeline Tasks

The LLM performs four distinct tasks in the project processing workflow and one in the chatbot workflow:

1. **Extract Projects** -- Splits multi-project comments into individual project descriptions
2. **Validate Project** -- Determines if extracted text is a real project (returns `is_valid` boolean)
3. **Generate Metadata** -- Produces title, description, short description, hashtags, and URL summaries
4. **Score Project** -- Rates idea quality (1-10) and implementation complexity (1-10)
5. **Chatbot Response** -- Generates a conversational answer grounded in retrieved project context

All tasks use structured JSON output enforced through prompt instructions. See the [Prompts](/ai-models/prompts) section for the full prompt templates.

## Source Files

- **LLM configuration**: `src/llm_config.py`
- **Prompt templates**: `src/workflows/prompts.py`
