---
title: AI Models Overview
description: Three-model NVIDIA Nemotron architecture powering project extraction, semantic search, and reranking in WAYWO.
navigation.icon: i-lucide-brain
---

# AI Models Overview

WAYWO uses three NVIDIA Nemotron models working together to transform raw Hacker News comments into structured, searchable project data. Each model handles a distinct stage of the pipeline.

## Three-Model Architecture

| Model | Role | Parameters | Output |
|-------|------|-----------|--------|
| `NVIDIA-Nemotron-3-Nano-30B` | Reasoning LLM | 30B | Structured JSON responses |
| `llama-embed-nemotron-8b` | Embedding | 8B | 4096-dim vectors |
| `llama-nemotron-rerank-1b-v2` | Reranker | 1.5B | Relevance scores |

## Why These Models

**Nemotron-3-Nano-30B** serves as the reasoning backbone. It handles all natural language understanding tasks -- extracting projects from comments, validating whether text describes a real project, generating metadata, scoring ideas, and powering the RAG chatbot. It runs via an OpenAI-compatible API endpoint, meaning no vendor lock-in and straightforward integration through LlamaIndex's `OpenAILike` client.

**llama-embed-nemotron-8b** generates high-dimensional (4096-dim) vector embeddings for each project. These embeddings enable semantic search -- finding projects by meaning rather than keyword matching. It runs as a standalone FastAPI microservice.

**llama-nemotron-rerank-1b-v2** is a cross-encoder reranker that refines search results. While embedding similarity gives a good first pass, the reranker scores each query-document pair directly, producing more accurate relevance rankings. It is lightweight at 1.5B parameters and also runs as a standalone FastAPI microservice.

## How They Work Together

The models participate at different stages of two core workflows:

### Project Processing Workflow

```
HN Comment
  --> [Nemotron-3-Nano-30B] Extract projects from comment
  --> [Nemotron-3-Nano-30B] Validate each project
  --> [Nemotron-3-Nano-30B] Generate metadata (title, description, tags)
  --> [Nemotron-3-Nano-30B] Score idea quality and complexity
  --> [llama-embed-nemotron-8b] Generate 4096-dim embedding
  --> Store in SQLite with sqlite-vector
```

### RAG Chatbot Workflow

```
User Query
  --> [llama-embed-nemotron-8b] Embed the query
  --> sqlite-vector cosine similarity search (top_k * 3 candidates)
  --> [llama-nemotron-rerank-1b-v2] Rerank candidates, select top_k
  --> [Nemotron-3-Nano-30B] Generate conversational response with context
```

## Deployment

All three models run as separate services, each on its own GPU. The main WAYWO backend communicates with them over HTTP:

| Service | Default URL | Protocol |
|---------|------------|----------|
| LLM (Nemotron-3-Nano-30B) | `http://192.168.6.19:8002/v1` | OpenAI-compatible API |
| Embedding Service | `http://192.168.5.96:8000` | REST (FastAPI) |
| Rerank Service | `http://192.168.5.173:8111` | REST (FastAPI) |

Service URLs are configurable via environment variables: `LLM_BASE_URL`, `EMBEDDING_URL`, and `RERANK_URL`.
