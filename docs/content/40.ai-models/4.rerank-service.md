---
title: Rerank Service
description: FastAPI microservice running llama-nemotron-rerank-1b-v2 for cross-encoder document reranking in the RAG pipeline.
navigation.icon: i-lucide-arrow-up-down
---

# Rerank Service

The rerank service runs the `nvidia/llama-nemotron-rerank-1b-v2` cross-encoder model as a standalone FastAPI microservice. It scores query-document pairs directly to produce more accurate relevance rankings than embedding similarity alone.

## Why a Reranker

Embedding-based similarity search is fast but approximate. It compares pre-computed vectors independently, meaning it cannot model the interaction between a query and a document. A cross-encoder reranker processes each query-document pair together through the full model, producing significantly more accurate relevance scores.

In WAYWO's RAG chatbot workflow, the reranker sits between the initial semantic search and the final LLM response generation:

```
Semantic search (top_k * 3 candidates) --> Reranker (select top_k) --> LLM response
```

This "retrieve then rerank" pattern gives both speed (vector search narrows the field) and accuracy (cross-encoder picks the best matches).

## Model Details

| Property | Value |
|----------|-------|
| Model | `nvidia/llama-nemotron-rerank-1b-v2` |
| Parameters | 1.5B |
| Type | Cross-encoder reranker |
| Max Sequence Length | 512 tokens (configurable up to 8192) |
| Languages | 26 languages supported |
| Framework | FastAPI + Uvicorn |

## API Endpoints

### POST /rerank

Rerank a list of documents by relevance to a query.

**Request:**
```json
{
  "query": "AI tools for productivity",
  "documents": [
    "A task management app built with React and Node.js",
    "An AI-powered writing assistant that helps with emails",
    "A weather widget for the desktop"
  ]
}
```

**Response:**
```json
{
  "scores": [0.45, 0.85, -0.12],
  "ranked_indices": [1, 0, 2]
}
```

- **scores** -- Raw logit scores for each document (higher means more relevant). These are not normalized to 0-1.
- **ranked_indices** -- Document indices sorted by relevance, most relevant first.

### GET /health

Returns service health status with model and device information.

```json
{
  "status": "healthy",
  "model_loaded": true,
  "device": "cuda:0"
}
```

## Client Configuration

The Python client in `src/rerank_client.py` connects to the service with built-in resilience:

| Setting | Value |
|---------|-------|
| Max Retries | 3 |
| Backoff Strategy | Exponential (`2^attempt` seconds) |
| Timeout | 60 seconds |
| Default URL | `http://192.168.5.173:8111` |

The client returns a `RerankResult` dataclass containing `scores` (list of floats) and `ranked_indices` (list of ints).

## Fallback Behavior

If the rerank service is unavailable or returns an error, the chatbot workflow **falls back to the original similarity order** from the embedding search. This means the chatbot remains functional even when the reranker is down -- results will be less precisely ranked but still relevant.

```python
except RerankError as e:
    logger.warning(f"Reranking failed, falling back to similarity order: {e}")
    # Use candidates in original similarity order
    for candidate in ev.candidates[:ev.top_k]:
        projects.append(candidate)
```

## Usage in the RAG Pipeline

The chatbot workflow retrieves `top_k * 3` candidates (default: 15) from semantic search, then uses the reranker to select the best `top_k` (default: 5). Each candidate is represented as `"{title}: {description}"` for reranking.

## Running the Service

```bash
cd services/rerank-service
uv sync              # install dependencies
uv run uvicorn main:app --host 0.0.0.0 --port 8001
```

## Source Files

- **Service**: `services/rerank-service/`
- **Client**: `src/rerank_client.py`
- **Chatbot workflow**: `src/workflows/waywo_chatbot_workflow.py`
- **Model reference**: [nvidia/llama-nemotron-rerank-1b-v2 on HuggingFace](https://huggingface.co/nvidia/llama-nemotron-rerank-1b-v2)
