---
title: First Run
description: A guided tour of your first WAYWO workflow -- from ingesting HN threads to chatting with the RAG bot.
navigation.icon: i-lucide-play
---

# First Run

With all services running (via [Docker Quickstart](/getting-started/quickstart) or [Manual Setup](/getting-started/manual-setup)), this guide walks you through the full pipeline: fetching HN posts, extracting projects, and using semantic search and the chatbot.

## The Pipeline

::steps

### Step 1: Fetch HN posts and comments

Trigger the post ingestion pipeline. This reads `waywo.yml`, fetches each HN thread, and downloads all top-level comments.

**Via API (Swagger UI or curl):**

```bash
curl -X POST http://localhost:8008/api/process-waywo-posts
```

You can optionally limit the scope for a quicker test:

```bash
curl -X POST http://localhost:8008/api/process-waywo-posts \
  -H "Content-Type: application/json" \
  -d '{"limit_posts": 2, "limit_comments": 10}'
```

This queues a Celery task. The response includes a `task_id` you can use to track progress.

### Step 2: Monitor task progress

Open **Flower** at [http://localhost:5555](http://localhost:5555) to watch the task execute. You will see the `process_waywo_posts` task appear in the task list. Click on it to see its status, runtime, and result.

You can also check Redis Insights at [http://localhost:7001](http://localhost:7001) to see the data being written.

::tip
The fetching step makes HTTP requests to the Hacker News API. For a post with hundreds of comments, this can take a few minutes as each comment is fetched individually.
::

### Step 3: Extract projects from comments

Once comments are stored, trigger the AI extraction pipeline. This sends each unprocessed comment through the Nemotron LLM to extract structured project data, validate it, and generate scores.

```bash
curl -X POST http://localhost:8008/api/process-waywo-comments
```

To limit processing for testing:

```bash
curl -X POST http://localhost:8008/api/process-waywo-comments \
  -H "Content-Type: application/json" \
  -d '{"limit": 20}'
```

::note
This step is the most time-intensive part of the pipeline. Each comment goes through an LLM-powered workflow with multiple steps: extraction, validation, scoring, hashtag generation, and embedding. Processing speed depends on your model server throughput.
::

### Step 4: Browse extracted projects

Open the Swagger UI at [http://localhost:8008/docs](http://localhost:8008/docs) and try the projects endpoints:

- **`GET /api/waywo-projects`** -- List all extracted projects with pagination
- **`GET /api/waywo-projects/{project_id}`** -- Get full details for a single project
- **`GET /api/waywo-projects/hashtags`** -- See all generated hashtags

Each project includes a title, description, URLs, tech stack, hashtags, idea score (1--10), complexity score (1--10), and a validity flag.

### Step 5: Try semantic search

Once projects have embeddings, you can search by meaning rather than keywords:

```bash
curl -X POST http://localhost:8008/api/semantic-search \
  -H "Content-Type: application/json" \
  -d '{"query": "developer tools for productivity", "limit": 5}'
```

The search endpoint generates a vector embedding of your query and finds the most similar projects using cosine similarity. By default, results are reranked with the rerank model for improved relevance.

::tip
Check embedding coverage with `GET /api/semantic-search/stats`. This shows how many of your projects have embeddings generated. You need embeddings for search to return results.
::

### Step 6: Chat with the RAG bot

Ask natural-language questions about the project index:

```bash
curl -X POST http://localhost:8008/api/waywo-chatbot \
  -H "Content-Type: application/json" \
  -d '{"query": "What are some interesting Rust projects people are building?"}'
```

The chatbot retrieves relevant projects via semantic search with reranking, then uses the LLM to generate a conversational response grounded in the actual project data. The response includes source project references so you can verify the information.

::

## Verifying the Stack

Use the admin health check to confirm all AI services are reachable:

```bash
curl http://localhost:8008/api/admin/services-health
```

This returns the status of the LLM, embedding, and rerank services, including available models and connection details.

For database statistics:

```bash
curl http://localhost:8008/api/admin/stats
```

## Observability

Open **Phoenix** at [http://localhost:6006](http://localhost:6006) to inspect LLM traces. Every call to the Nemotron model during extraction and chat is captured with full request/response details, token counts, and latency. This is invaluable for debugging prompt behavior and understanding pipeline performance.

## Next Steps

- Browse the full [API reference](/api/overview) for all 45+ endpoints
- Learn about the [data pipeline](/pipeline/overview) in depth
- Explore the [AI model stack](/ai-models/overview) and prompt engineering
- Check out the [Jupyter notebooks](http://localhost:8888) for interactive data exploration
