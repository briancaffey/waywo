---
title: Prerequisites
description: Required software, hardware, and environment variables needed to run WAYWO.
navigation.icon: i-lucide-clipboard-check
---

# Prerequisites

## Required Software

### Docker Setup (Recommended)

For the standard Docker-based deployment, you need:

- **Docker** >= 24.0
- **Docker Compose** >= 2.20

This is the recommended approach. All Python dependencies, Redis, and supporting services are handled by the containers.

### Manual Setup (Development)

If you prefer running services directly on your machine:

- **Python** >= 3.12
- **Node.js** >= 20 (for the Nuxt 4 frontend)
- **Redis** (running separately)

## AI Model Services

WAYWO relies on three NVIDIA Nemotron model endpoints. These run outside the Docker Compose stack and must be accessible over your network:

| Model | Purpose | Default Port |
|-------|---------|-------------|
| NVIDIA Nemotron-3-Nano-30B | LLM for extraction, validation, scoring, and chat | 8002 |
| llama-embed-nemotron-8b | Vector embedding generation | 8000 |
| llama-nemotron-rerank-1b | Search result reranking | 8111 |

::note
The model endpoints are configured via environment variables. The default values in `docker-compose.yml` point to local network IPs. You will need to update these to match the hosts where your models are running.
::

## Environment Variables

WAYWO uses the following environment variables, defined in the `x-common-env` anchor of `docker-compose.yml`:

| Variable | Default | Description |
|----------|---------|-------------|
| `CELERY_BROKER_URL` | `redis://redis:6379/0` | Redis URL for the Celery task broker |
| `CELERY_RESULT_BACKEND` | `redis://redis:6379/0` | Redis URL for Celery result storage |
| `REDIS_URL` | `redis://redis:6379/0` | Redis connection URL for application caching |
| `NAT_SERVICE_URL` | `http://nat:8080` | URL of the NAT (Nemotron Agent Toolkit) service |
| `FIRECRAWL_URL` | `http://host.docker.internal:3002` | Firecrawl service URL for web scraping |
| `EMBEDDING_URL` | `http://192.168.5.96:8000` | Embedding model server URL |
| `RERANK_URL` | `http://192.168.5.173:8111` | Rerank model server URL |
| `LLM_BASE_URL` | `http://192.168.6.19:8002/v1` | LLM server URL (OpenAI-compatible API) |
| `LLM_MODEL_NAME` | `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16` | Model identifier for the LLM |
| `DATA_DIR` | `/app/data` | Directory for SQLite database and data files |
| `LOG_LEVEL` | `DEBUG` | Application log level |
| `PHOENIX_HOST` | `phoenix` | Hostname of the Phoenix observability service |
| `PHOENIX_PORT` | `6006` | Port of the Phoenix observability service |

::warning
The `EMBEDDING_URL`, `RERANK_URL`, and `LLM_BASE_URL` values in the default `docker-compose.yml` are hardcoded to specific local network IPs (e.g., `192.168.x.x`). You **must** update these to point to the machines where your NVIDIA models are actually running before starting the stack.
::

The NAT service also accepts additional environment variables passed at the service level:

| Variable | Default | Description |
|----------|---------|-------------|
| `NVIDIA_API_KEY` | `not-needed` | NVIDIA API key (only needed for cloud-hosted models) |
