{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# NeMo DataDesigner ‚Äî Pipeline Test Notebook\n",
    "\n",
    "Interactive testing of the NDD pipeline before wiring it into the full stack.\n",
    "\n",
    "1. **Setup** ‚Äî imports, env var verification, provider/model instantiation\n",
    "2. **Tag data** ‚Äî load existing projects, build co-occurrence map\n",
    "3. **Pipeline preview** ‚Äî run preview with 2-3 records\n",
    "4. **Validate output** ‚Äî check columns match WaywoProjectDB fields\n",
    "5. **Prompt iteration** ‚Äî tweak and re-run\n",
    "6. **Embedding test** ‚Äî generate embedding for a row\n",
    "7. **Full save test** ‚Äî save a generated project to DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/app')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_BASE_URL:   http://192.168.6.19:8002/v1\n",
      "LLM_MODEL_NAME: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n",
      "LLM_API_KEY: not-needed\n",
      "EMBEDDING_URL:  http://192.168.5.96:8000\n"
     ]
    }
   ],
   "source": [
    "# Verify env vars\n",
    "from src.settings import LLM_BASE_URL, LLM_MODEL_NAME, LLM_API_KEY, EMBEDDING_URL\n",
    "\n",
    "print(f'LLM_BASE_URL:   {LLM_BASE_URL}')\n",
    "print(f'LLM_MODEL_NAME: {LLM_MODEL_NAME}')\n",
    "print(f'LLM_API_KEY:    {LLM_API_KEY[:10]}...' if len(LLM_API_KEY) > 10 else f'LLM_API_KEY: {LLM_API_KEY}')\n",
    "print(f'EMBEDDING_URL:  {EMBEDDING_URL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ndd-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[01:39:40] [INFO] NDD provider: waywo-llm -> http://192.168.6.19:8002/v1\n",
      "[01:39:40] [INFO] NDD models: ['waywo-creative', 'waywo-structured', 'waywo-judge'], model=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16, creative temp=0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataDesigner initialized\n"
     ]
    }
   ],
   "source": [
    "# Build provider and models\n",
    "from src.ndd_config import build_ndd_provider, build_ndd_models\n",
    "from data_designer.interface.data_designer import DataDesigner\n",
    "\n",
    "provider = build_ndd_provider()\n",
    "models = build_ndd_models(creativity=0.85)\n",
    "\n",
    "dd = DataDesigner(\n",
    "    model_providers=[provider],\n",
    "    artifact_path='/app/data/ndd_artifacts',\n",
    ")\n",
    "print('DataDesigner initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tag-header",
   "metadata": {},
   "source": [
    "## 2. Tag Data ‚Äî Co-occurrence Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-projects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 444 valid projects\n",
      "Unique tags: 915\n",
      "Sample: ['3d', '3dprinting', '6502', 'ableton', 'abtesting', 'academic', 'access-control', 'accessibility', 'accounting', 'actions', 'activitypub', 'adblocking', 'adtech', 'adultart', 'advisory', 'agegating', 'agent', 'agents', 'aggregation', 'aggregator']\n"
     ]
    }
   ],
   "source": [
    "from src.db.projects import get_all_projects, get_all_hashtags\n",
    "from src.ndd_pipeline import build_tag_cooccurrence\n",
    "\n",
    "# Load all valid projects\n",
    "projects = get_all_projects(is_valid=True)\n",
    "print(f'Loaded {len(projects)} valid projects')\n",
    "\n",
    "# Get all unique tags\n",
    "all_tags = get_all_hashtags()\n",
    "print(f'Unique tags: {len(all_tags)}')\n",
    "print(f'Sample: {all_tags[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cooccurrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:40:03] [INFO] Built tag co-occurrence map: 915 tags, avg 7 co-tags each\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags with co-occurrence data: 915\n",
      "  ai: [('productivity', 33), ('opensource', 20), ('saas', 20), ('education', 12), ('llm', 12)]\n",
      "  web: [('saas', 3), ('productivity', 3), ('javascript', 2), ('monitoring', 2), ('uptime', 2)]\n",
      "  python: [('opensource', 4), ('ai', 4), ('rag', 2), ('django', 1), ('activitypub', 1)]\n",
      "  saas: [('productivity', 22), ('ai', 20), ('monitoring', 5), ('devops', 4), ('web', 3)]\n",
      "  open-source: [('typescript', 1), ('linq', 1), ('database', 1), ('query', 1), ('emulator', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Build co-occurrence map\n",
    "cooccurrence = build_tag_cooccurrence(projects)\n",
    "print(f'Tags with co-occurrence data: {len(cooccurrence)}')\n",
    "\n",
    "# Inspect top tags\n",
    "for tag in ['ai', 'web', 'python', 'saas', 'open-source']:\n",
    "    if tag in cooccurrence:\n",
    "        top5 = cooccurrence[tag][:5]\n",
    "        print(f'  {tag}: {top5}')\n",
    "    else:\n",
    "        print(f'  {tag}: (not in co-occurrence data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preview-header",
   "metadata": {},
   "source": [
    "## 3. Pipeline Preview\n",
    "\n",
    "Generate 2-3 records to test the pipeline without a full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "build-config",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:40:09] [INFO] Built NDD pipeline: 4 samplers, 1 LLM text, 1 structured, 1 judge, 4 expression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline columns:\n",
      "  [sampler] primary_tag\n",
      "  [sampler] secondary_tags\n",
      "  [sampler] target_audience\n",
      "  [sampler] target_complexity\n",
      "  [llm-text] project_idea\n",
      "  [llm-structured] metadata\n",
      "  [llm-judge] idea_quality\n",
      "  [expression] title\n",
      "  [expression] short_description\n",
      "  [expression] description\n",
      "  [expression] hashtags\n"
     ]
    }
   ],
   "source": [
    "from src.ndd_pipeline import build_pipeline_config\n",
    "\n",
    "# Build config with seed tags\n",
    "config = build_pipeline_config(\n",
    "    models=models,\n",
    "    seed_tags=['ai', 'python', 'developer-tools'],\n",
    "    tag_cooccurrence=cooccurrence,\n",
    "    all_tags=all_tags,\n",
    ")\n",
    "\n",
    "# Show pipeline structure\n",
    "print('Pipeline columns:')\n",
    "for col in config.get_column_configs():\n",
    "    print(f'  [{col.column_type}] {col.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "run-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:40:11] [INFO] üëÄ Preview generation in progress\n",
      "[01:40:15] [INFO] ‚úÖ Validation passed\n",
      "[01:40:15] [INFO] ‚õìÔ∏è Sorting column configs into a Directed Acyclic Graph\n",
      "[01:40:15] [INFO] ü©∫ Running health checks for models...\n",
      "[01:40:15] [INFO]   |-- ‚è≠Ô∏è  Skipping health check for model alias 'waywo-judge' (skip_health_check=True)\n",
      "[01:40:15] [INFO]   |-- ‚è≠Ô∏è  Skipping health check for model alias 'waywo-structured' (skip_health_check=True)\n",
      "[01:40:15] [INFO]   |-- ‚è≠Ô∏è  Skipping health check for model alias 'waywo-creative' (skip_health_check=True)\n",
      "[01:40:15] [INFO] üé≤ Preparing samplers to generate 2 records across 4 columns\n",
      "[01:40:17] [INFO] üìù llm-text model config for column 'project_idea'\n",
      "[01:40:17] [INFO]   |-- model: 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'\n",
      "[01:40:17] [INFO]   |-- model alias: 'waywo-creative'\n",
      "[01:40:17] [INFO]   |-- model provider: 'waywo-llm'\n",
      "[01:40:17] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=0.85, max_tokens=2048\n",
      "[01:40:17] [INFO] üêô Processing llm-text column 'project_idea' with 4 concurrent workers\n",
      "[01:40:17] [INFO] üß≠ llm-text column 'project_idea' will report progress every 1 record(s).\n",
      "[01:40:51] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-31' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "/usr/local/lib/python3.12/asyncio/base_events.py:716: RuntimeWarning: coroutine 'ServiceLogging.async_service_success_hook' was never awaited\n",
      "  self._ready.clear()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[01:40:51] [INFO]   |-- üê• llm-text column 'project_idea' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.03 rec/s, eta 33.5s\n",
      "[01:41:17] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-32' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:41:17] [INFO]   |-- üêî llm-text column 'project_idea' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.03 rec/s, eta 0.0s\n",
      "[01:41:17] [INFO] üóÇÔ∏è llm-structured model config for column 'metadata'\n",
      "[01:41:17] [INFO]   |-- model: 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'\n",
      "[01:41:17] [INFO]   |-- model alias: 'waywo-structured'\n",
      "[01:41:17] [INFO]   |-- model provider: 'waywo-llm'\n",
      "[01:41:17] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=0.10, max_tokens=2048\n",
      "[01:41:17] [INFO] üêô Processing llm-structured column 'metadata' with 4 concurrent workers\n",
      "[01:41:17] [INFO] üß≠ llm-structured column 'metadata' will report progress every 1 record(s).\n",
      "[01:41:42] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-37' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:41:42] [INFO]   |-- üê• llm-structured column 'metadata' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.04 rec/s, eta 25.1s\n",
      "[01:41:47] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-38' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:41:47] [INFO]   |-- üêî llm-structured column 'metadata' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.07 rec/s, eta 0.0s\n",
      "[01:41:47] [INFO] ‚öñÔ∏è llm-judge model config for column 'idea_quality'\n",
      "[01:41:47] [INFO]   |-- model: 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'\n",
      "[01:41:47] [INFO]   |-- model alias: 'waywo-judge'\n",
      "[01:41:47] [INFO]   |-- model provider: 'waywo-llm'\n",
      "[01:41:47] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=0.20, max_tokens=1024\n",
      "[01:41:47] [INFO] üêô Processing llm-judge column 'idea_quality' with 4 concurrent workers\n",
      "[01:41:47] [INFO] üß≠ llm-judge column 'idea_quality' will report progress every 1 record(s).\n",
      "[01:42:02] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-43' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:42:03] [INFO]   |-- üê• llm-judge column 'idea_quality' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.06 rec/s, eta 15.8s\n",
      "[01:42:07] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-44' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:42:07] [INFO]   |-- üêî llm-judge column 'idea_quality' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.10 rec/s, eta 0.0s\n",
      "[01:42:07] [INFO] üß© Generating column `title` from expression\n",
      "[01:42:07] [INFO] üß© Generating column `short_description` from expression\n",
      "[01:42:07] [INFO] üß© Generating column `description` from expression\n",
      "[01:42:07] [INFO] üß© Generating column `hashtags` from expression\n",
      "[01:42:07] [INFO] üìä Model usage summary:\n",
      "{\n",
      "    \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\": {\n",
      "        \"token_usage\": {\n",
      "            \"input_tokens\": 1927,\n",
      "            \"output_tokens\": 897,\n",
      "            \"total_tokens\": 2824\n",
      "        },\n",
      "        \"request_usage\": {\n",
      "            \"successful_requests\": 2,\n",
      "            \"failed_requests\": 0,\n",
      "            \"total_requests\": 2\n",
      "        },\n",
      "        \"tokens_per_second\": 25,\n",
      "        \"requests_per_minute\": 1\n",
      "    }\n",
      "}\n",
      "[01:42:07] [INFO] üìê Measuring dataset column statistics:\n",
      "/app/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:1040: RuntimeWarning: coroutine 'TelemetryHandler._timer_loop' was never awaited\n",
      "  nb = type(block)(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[01:42:07] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task cancelling name='Task-46' coro=<TelemetryHandler._timer_loop() running at /app/.venv/lib/python3.12/site-packages/data_designer/engine/models/telemetry.py:309>>\n",
      "[01:42:07] [INFO]   |-- üé≤ column: 'primary_tag'\n",
      "[01:42:07] [INFO]   |-- üé≤ column: 'secondary_tags'\n",
      "[01:42:07] [INFO]   |-- üé≤ column: 'target_audience'\n",
      "[01:42:07] [INFO]   |-- üé≤ column: 'target_complexity'\n",
      "[01:42:07] [INFO]   |-- üìù column: 'project_idea'\n",
      "[01:42:07] [INFO]   |-- üóÇÔ∏è column: 'metadata'\n",
      "[01:42:07] [INFO]   |-- ‚öñÔ∏è column: 'idea_quality'\n",
      "[01:42:07] [INFO]   |-- üß© column: 'title'\n",
      "[01:42:07] [INFO]   |-- üß© column: 'short_description'\n",
      "[01:42:07] [INFO]   |-- üß© column: 'description'\n",
      "[01:42:07] [INFO]   |-- üß© column: 'hashtags'\n",
      "[01:42:07] [INFO] üéÜ Preview complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 records\n",
      "Columns: ['primary_tag', 'secondary_tags', 'target_audience', 'target_complexity', 'project_idea', 'metadata', 'idea_quality', 'title', 'short_description', 'description', 'hashtags']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_tag</th>\n",
       "      <th>secondary_tags</th>\n",
       "      <th>target_audience</th>\n",
       "      <th>target_complexity</th>\n",
       "      <th>project_idea</th>\n",
       "      <th>metadata</th>\n",
       "      <th>idea_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>short_description</th>\n",
       "      <th>description</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai</td>\n",
       "      <td>llm</td>\n",
       "      <td>creative technologists</td>\n",
       "      <td>7</td>\n",
       "      <td>\\n**Project Overview**  \\nCreate **‚ÄúMuseWeaver...</td>\n",
       "      <td>{'title': 'MuseWeaver AI Storytelling Platform...</td>\n",
       "      <td>{'idea_score': {'reasoning': 'The concept comb...</td>\n",
       "      <td>MuseWeaver AI Storytelling Platform</td>\n",
       "      <td>AI-driven collaborative narrative creation wit...</td>\n",
       "      <td>MuseWeaver merges constraint‚Äëguided LLM genera...</td>\n",
       "      <td>['ai', 'interactive-storytelling', 'collaborat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python</td>\n",
       "      <td>ai</td>\n",
       "      <td>frontend developers</td>\n",
       "      <td>9</td>\n",
       "      <td>\\n**Project Overview ‚Äì ‚ÄúPromptUI‚Äù**  \\nPromptU...</td>\n",
       "      <td>{'title': 'PromptUI AI UI Generator', 'short_d...</td>\n",
       "      <td>{'idea_score': {'reasoning': 'The concept merg...</td>\n",
       "      <td>PromptUI AI UI Generator</td>\n",
       "      <td>AI-powered CLI that generates UI components fr...</td>\n",
       "      <td>PromptUI parses natural‚Äëlanguage UI specs or d...</td>\n",
       "      <td>['python', 'ai-code-generation', 'frontend-aut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_tag secondary_tags         target_audience  target_complexity  \\\n",
       "0          ai            llm  creative technologists                  7   \n",
       "1      python             ai     frontend developers                  9   \n",
       "\n",
       "                                        project_idea  \\\n",
       "0  \\n**Project Overview**  \\nCreate **‚ÄúMuseWeaver...   \n",
       "1  \\n**Project Overview ‚Äì ‚ÄúPromptUI‚Äù**  \\nPromptU...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'title': 'MuseWeaver AI Storytelling Platform...   \n",
       "1  {'title': 'PromptUI AI UI Generator', 'short_d...   \n",
       "\n",
       "                                        idea_quality  \\\n",
       "0  {'idea_score': {'reasoning': 'The concept comb...   \n",
       "1  {'idea_score': {'reasoning': 'The concept merg...   \n",
       "\n",
       "                                 title  \\\n",
       "0  MuseWeaver AI Storytelling Platform   \n",
       "1             PromptUI AI UI Generator   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  AI-driven collaborative narrative creation wit...   \n",
       "1  AI-powered CLI that generates UI components fr...   \n",
       "\n",
       "                                         description  \\\n",
       "0  MuseWeaver merges constraint‚Äëguided LLM genera...   \n",
       "1  PromptUI parses natural‚Äëlanguage UI specs or d...   \n",
       "\n",
       "                                            hashtags  \n",
       "0  ['ai', 'interactive-storytelling', 'collaborat...  \n",
       "1  ['python', 'ai-code-generation', 'frontend-aut...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run preview (this calls the LLM ‚Äî may take 30-60s)\n",
    "preview = dd.preview(config, num_records=2)\n",
    "df = preview.dataset\n",
    "\n",
    "print(f'Generated {len(df)} records')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inspect-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Row 0 ===\n",
      "primary_tag:      ai\n",
      "secondary_tags:   llm\n",
      "target_audience:  creative technologists\n",
      "target_complexity:7\n",
      "\n",
      "--- project_idea (first 300 chars) ---\n",
      "\n",
      "**Project Overview**  \n",
      "Create **‚ÄúMuseWeaver,‚Äù** an AI‚Äëdriven collaborative storytelling platform that lets creative technologists co‚Äëauthor interactive narrative experiences (e.g., choose‚Äëyour‚Äëown‚Äëadventure games, AR‚Äëenabled tales, or web‚Äëbased interactive comics) in real time. The core problem it \n",
      "\n",
      "--- metadata ---\n",
      "{'title': 'MuseWeaver AI Storytelling Platform', 'short_description': 'AI-driven collaborative narrative creation with real-time branching', 'description': 'MuseWeaver merges constraint‚Äëguided LLM generation with a semantic branching graph, enabling real‚Äëtime co‚Äëauthoring and instant export to game engines or AR web experiences. It solves fragmentation in interactive story workflows and lets small teams ship immersive narratives quickly.', 'hashtags': ['ai', 'interactive-storytelling', 'collaborative-narrative', 'game-dev']}\n",
      "\n",
      "--- Extracted fields ---\n",
      "title:             MuseWeaver AI Storytelling Platform\n",
      "short_description: AI-driven collaborative narrative creation with real-time branching\n",
      "description:       MuseWeaver merges constraint‚Äëguided LLM generation with a semantic branching graph, enabling real‚Äëtime co‚Äëauthoring and instant export to game engines or AR web experiences. It solves fragmentation in\n",
      "hashtags:          ['ai', 'interactive-storytelling', 'collaborative-narrative', 'game-dev']\n",
      "\n",
      "--- Scores ---\n",
      "idea_quality:      {'idea_score': {'reasoning': 'The concept combines several cutting‚Äëedge areas‚ÄîLLM‚Äëdriven narrative generation, semantic branching graphs, and seamless export to game engines and AR web platforms‚Äîinto a unified collaborative authoring tool. It addresses a clear pain point (fragmented interactive‚Äëstory workflows) and offers a novel workflow that enables small teams to produce immersive, real‚Äëtime co‚Äëauthored stories. While it builds on existing technologies, the integration and real‚Äëtime branching logic create a distinctive, high‚Äëimpact product that stands out from typical story‚Äëgeneration tools.', 'score': 8}, 'complexity_score': {'reasoning': 'Building MuseWeaver requires integrating large language model APIs, a dynamic semantic graph engine, real‚Äëtime collaboration infrastructure, and export pipelines for multiple game/AR engines. This involves distributed system design, low‚Äëlatency networking, and advanced algorithmic orchestration, placing it well above a moderate complexity level but below the most research‚Äëintensive projects.', 'score': 7}}\n"
     ]
    }
   ],
   "source": [
    "# Inspect a generated row in detail\n",
    "if len(df) > 0:\n",
    "    row = df.iloc[0]\n",
    "    print(f'=== Row 0 ===')\n",
    "    print(f'primary_tag:      {row.get(\"primary_tag\")}')\n",
    "    print(f'secondary_tags:   {row.get(\"secondary_tags\")}')\n",
    "    print(f'target_audience:  {row.get(\"target_audience\")}')\n",
    "    print(f'target_complexity:{row.get(\"target_complexity\")}')\n",
    "    print(f'\\n--- project_idea (first 300 chars) ---')\n",
    "    print(str(row.get('project_idea', ''))[:300])\n",
    "    print(f'\\n--- metadata ---')\n",
    "    print(row.get('metadata'))\n",
    "    print(f'\\n--- Extracted fields ---')\n",
    "    print(f'title:             {row.get(\"title\")}')\n",
    "    print(f'short_description: {row.get(\"short_description\")}')\n",
    "    print(f'description:       {str(row.get(\"description\", \"\"))[:200]}')\n",
    "    print(f'hashtags:          {row.get(\"hashtags\")}')\n",
    "    print(f'\\n--- Scores ---')\n",
    "    print(f'idea_quality:      {row.get(\"idea_quality\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate-header",
   "metadata": {},
   "source": [
    "## 4. Validate Output\n",
    "\n",
    "Check that output columns can map to `WaywoProjectDB` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "validate-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required fields in output...\n",
      "  OK: title = MuseWeaver AI Storytelling Platform\n",
      "  OK: short_description = AI-driven collaborative narrative creation with real-time br\n",
      "  OK: description = MuseWeaver merges constraint‚Äëguided LLM generation with a se\n",
      "  OK: hashtags = ['ai', 'interactive-storytelling', 'collaborative-narrative'\n",
      "\n",
      "Checking scores...\n",
      "  idea_quality raw: {'idea_score': {'reasoning': 'The concept combines several cutting‚Äëedge areas‚ÄîLLM‚Äëdriven narrative generation, semantic branching graphs, and seamless export to game engines and AR web platforms‚Äîinto a unified collaborative authoring tool. It addresses a clear pain point (fragmented interactive‚Äëstory workflows) and offers a novel workflow that enables small teams to produce immersive, real‚Äëtime co‚Äëauthored stories. While it builds on existing technologies, the integration and real‚Äëtime branching logic create a distinctive, high‚Äëimpact product that stands out from typical story‚Äëgeneration tools.', 'score': 8}, 'complexity_score': {'reasoning': 'Building MuseWeaver requires integrating large language model APIs, a dynamic semantic graph engine, real‚Äëtime collaboration infrastructure, and export pipelines for multiple game/AR engines. This involves distributed system design, low‚Äëlatency networking, and advanced algorithmic orchestration, placing it well above a moderate complexity level but below the most research‚Äëintensive projects.', 'score': 7}} (type: dict)\n",
      "  idea_score:       {'reasoning': 'The concept combines several cutting‚Äëedge areas‚ÄîLLM‚Äëdriven narrative generation, semantic branching graphs, and seamless export to game engines and AR web platforms‚Äîinto a unified collaborative authoring tool. It addresses a clear pain point (fragmented interactive‚Äëstory workflows) and offers a novel workflow that enables small teams to produce immersive, real‚Äëtime co‚Äëauthored stories. While it builds on existing technologies, the integration and real‚Äëtime branching logic create a distinctive, high‚Äëimpact product that stands out from typical story‚Äëgeneration tools.', 'score': 8}\n",
      "  complexity_score: {'reasoning': 'Building MuseWeaver requires integrating large language model APIs, a dynamic semantic graph engine, real‚Äëtime collaboration infrastructure, and export pipelines for multiple game/AR engines. This involves distributed system design, low‚Äëlatency networking, and advanced algorithmic orchestration, placing it well above a moderate complexity level but below the most research‚Äëintensive projects.', 'score': 7}\n"
     ]
    }
   ],
   "source": [
    "required_fields = ['title', 'short_description', 'description', 'hashtags']\n",
    "\n",
    "print('Checking required fields in output...')\n",
    "for field in required_fields:\n",
    "    present = field in df.columns\n",
    "    sample = str(df.iloc[0].get(field, 'MISSING'))[:60] if present and len(df) > 0 else 'N/A'\n",
    "    status = 'OK' if present else 'MISSING'\n",
    "    print(f'  {status}: {field} = {sample}')\n",
    "\n",
    "# Check scores\n",
    "print('\\nChecking scores...')\n",
    "if 'idea_quality' in df.columns and len(df) > 0:\n",
    "    quality = df.iloc[0]['idea_quality']\n",
    "    print(f'  idea_quality raw: {quality} (type: {type(quality).__name__})')\n",
    "    # Scores might be nested in the quality dict/string\n",
    "    if isinstance(quality, str):\n",
    "        try:\n",
    "            quality = json.loads(quality)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    if isinstance(quality, dict):\n",
    "        print(f'  idea_score:       {quality.get(\"idea_score\")}')\n",
    "        print(f'  complexity_score: {quality.get(\"complexity_score\")}')\n",
    "    else:\n",
    "        print(f'  (unexpected format ‚Äî may need parsing adjustment)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iterate-header",
   "metadata": {},
   "source": [
    "## 5. Prompt Iteration\n",
    "\n",
    "Tweak prompts by rebuilding the config with different parameters and re-running preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "iterate-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:42:52] [INFO] NDD models: ['waywo-creative', 'waywo-structured', 'waywo-judge'], model=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16, creative temp=1.1\n",
      "[01:42:52] [INFO] Built NDD pipeline: 4 samplers, 1 LLM text, 1 structured, 1 judge, 4 expression\n",
      "[01:42:52] [INFO] üëÄ Preview generation in progress\n",
      "[01:42:52] [INFO] ‚úÖ Validation passed\n",
      "[01:42:52] [INFO] ‚õìÔ∏è Sorting column configs into a Directed Acyclic Graph\n",
      "[01:42:52] [INFO] ü©∫ Running health checks for models...\n",
      "[01:42:52] [INFO]   |-- ‚è≠Ô∏è  Skipping health check for model alias 'waywo-judge' (skip_health_check=True)\n",
      "[01:42:52] [INFO]   |-- ‚è≠Ô∏è  Skipping health check for model alias 'waywo-structured' (skip_health_check=True)\n",
      "[01:42:52] [INFO]   |-- ‚è≠Ô∏è  Skipping health check for model alias 'waywo-creative' (skip_health_check=True)\n",
      "[01:42:52] [INFO] üé≤ Preparing samplers to generate 2 records across 4 columns\n",
      "[01:42:52] [INFO] üìù llm-text model config for column 'project_idea'\n",
      "[01:42:52] [INFO]   |-- model: 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'\n",
      "[01:42:52] [INFO]   |-- model alias: 'waywo-creative'\n",
      "[01:42:52] [INFO]   |-- model provider: 'waywo-llm'\n",
      "[01:42:52] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=1.10, max_tokens=2048\n",
      "[01:42:52] [INFO] üêô Processing llm-text column 'project_idea' with 4 concurrent workers\n",
      "[01:42:52] [INFO] üß≠ llm-text column 'project_idea' will report progress every 1 record(s).\n",
      "[01:43:36] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-61' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "/usr/local/lib/python3.12/asyncio/base_events.py:716: RuntimeWarning: coroutine 'ServiceLogging.async_service_success_hook' was never awaited\n",
      "  self._ready.clear()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[01:43:36] [INFO]   |-- ‚õÖ llm-text column 'project_idea' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.02 rec/s, eta 43.7s\n",
      "[01:44:25] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-64' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:44:25] [INFO]   |-- ‚òÄÔ∏è llm-text column 'project_idea' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.02 rec/s, eta 0.0s\n",
      "[01:44:25] [INFO] üóÇÔ∏è llm-structured model config for column 'metadata'\n",
      "[01:44:25] [INFO]   |-- model: 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'\n",
      "[01:44:25] [INFO]   |-- model alias: 'waywo-structured'\n",
      "[01:44:25] [INFO]   |-- model provider: 'waywo-llm'\n",
      "[01:44:25] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=0.10, max_tokens=2048\n",
      "[01:44:25] [INFO] üêô Processing llm-structured column 'metadata' with 4 concurrent workers\n",
      "[01:44:25] [INFO] üß≠ llm-structured column 'metadata' will report progress every 1 record(s).\n",
      "[01:44:50] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-69' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:44:50] [INFO]   |-- üåó llm-structured column 'metadata' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.04 rec/s, eta 25.3s\n",
      "[01:45:09] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-70' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:45:09] [INFO]   |-- üåï llm-structured column 'metadata' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.05 rec/s, eta 0.0s\n",
      "[01:45:09] [INFO] ‚öñÔ∏è llm-judge model config for column 'idea_quality'\n",
      "[01:45:09] [INFO]   |-- model: 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'\n",
      "[01:45:09] [INFO]   |-- model alias: 'waywo-judge'\n",
      "[01:45:09] [INFO]   |-- model provider: 'waywo-llm'\n",
      "[01:45:09] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=0.20, max_tokens=1024\n",
      "[01:45:09] [INFO] üêô Processing llm-judge column 'idea_quality' with 4 concurrent workers\n",
      "[01:45:09] [INFO] üß≠ llm-judge column 'idea_quality' will report progress every 1 record(s).\n",
      "[01:45:34] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-75' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:45:34] [INFO]   |-- üê• llm-judge column 'idea_quality' progress: 1/2 (50%) complete, 1 ok, 0 failed, 0.04 rec/s, eta 24.5s\n",
      "[01:45:35] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-76' coro=<ServiceLogging.async_service_success_hook() running at /app/.venv/lib/python3.12/site-packages/litellm/_service_logger.py:107>>\n",
      "[01:45:35] [INFO]   |-- üêî llm-judge column 'idea_quality' progress: 2/2 (100%) complete, 2 ok, 0 failed, 0.08 rec/s, eta 0.0s\n",
      "[01:45:35] [INFO] üß© Generating column `title` from expression\n",
      "[01:45:35] [INFO] üß© Generating column `short_description` from expression\n",
      "[01:45:35] [INFO] üß© Generating column `description` from expression\n",
      "[01:45:35] [INFO] üß© Generating column `hashtags` from expression\n",
      "[01:45:35] [INFO] üìä Model usage summary:\n",
      "{\n",
      "    \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\": {\n",
      "        \"token_usage\": {\n",
      "            \"input_tokens\": 939,\n",
      "            \"output_tokens\": 605,\n",
      "            \"total_tokens\": 1544\n",
      "        },\n",
      "        \"request_usage\": {\n",
      "            \"successful_requests\": 1,\n",
      "            \"failed_requests\": 0,\n",
      "            \"total_requests\": 1\n",
      "        },\n",
      "        \"tokens_per_second\": 9,\n",
      "        \"requests_per_minute\": 0\n",
      "    }\n",
      "}\n",
      "[01:45:35] [INFO] üìê Measuring dataset column statistics:\n",
      "/app/.venv/lib/python3.12/site-packages/pyarrow/pandas_compat.py:808: RuntimeWarning: coroutine 'TelemetryHandler._timer_loop' was never awaited\n",
      "  result = pa.lib.table_to_blocks(options, table, categories,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[01:45:35] [ERROR] Task was destroyed but it is pending!\n",
      "task: <Task cancelling name='Task-78' coro=<TelemetryHandler._timer_loop() running at /app/.venv/lib/python3.12/site-packages/data_designer/engine/models/telemetry.py:309>>\n",
      "[01:45:35] [INFO]   |-- üé≤ column: 'primary_tag'\n",
      "[01:45:35] [INFO]   |-- üé≤ column: 'secondary_tags'\n",
      "[01:45:35] [INFO]   |-- üé≤ column: 'target_audience'\n",
      "[01:45:35] [INFO]   |-- üé≤ column: 'target_complexity'\n",
      "[01:45:35] [INFO]   |-- üìù column: 'project_idea'\n",
      "[01:45:35] [INFO]   |-- üóÇÔ∏è column: 'metadata'\n",
      "[01:45:35] [INFO]   |-- ‚öñÔ∏è column: 'idea_quality'\n",
      "[01:45:35] [INFO]   |-- üß© column: 'title'\n",
      "[01:45:35] [INFO]   |-- üß© column: 'short_description'\n",
      "[01:45:35] [INFO]   |-- üß© column: 'description'\n",
      "[01:45:35] [INFO]   |-- üß© column: 'hashtags'\n",
      "[01:45:35] [INFO] üôå Preview complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-creativity results (2 rows):\n",
      "\n",
      "  [0] Data Hunt ‚Äî Gamified data discovery and model training via NFTs\n",
      "      Tags: ['gaming', 'machine-learning', 'data-science', 'blockchain', 'web3']\n",
      "\n",
      "  [1] Dungeon Companion AI ‚Äî AI companion that learns playstyle and adapts dungeon difficulty\n",
      "      Tags: ['gaming', 'roguelike-game', 'artificial-intelligence', 'reinforcement-learning']\n"
     ]
    }
   ],
   "source": [
    "# Try with different seed tags and higher creativity\n",
    "models_wild = build_ndd_models(creativity=1.1)\n",
    "\n",
    "config_wild = build_pipeline_config(\n",
    "    models=models_wild,\n",
    "    seed_tags=['blockchain', 'gaming'],\n",
    "    tag_cooccurrence=cooccurrence,\n",
    "    all_tags=all_tags,\n",
    ")\n",
    "\n",
    "dd_wild = DataDesigner(\n",
    "    model_providers=[provider],\n",
    "    artifact_path='/app/data/ndd_artifacts',\n",
    ")\n",
    "\n",
    "preview_wild = dd_wild.preview(config_wild, num_records=2)\n",
    "df_wild = preview_wild.dataset\n",
    "\n",
    "print(f'High-creativity results ({len(df_wild)} rows):')\n",
    "for i, row in df_wild.iterrows():\n",
    "    print(f'\\n  [{i}] {row.get(\"title\", \"?\")} ‚Äî {row.get(\"short_description\", \"?\")}')\n",
    "    print(f'      Tags: {row.get(\"hashtags\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-header",
   "metadata": {},
   "source": [
    "## 6. Embedding Test\n",
    "\n",
    "Take a generated row and run it through the embedding pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "embedding-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:46:02] [INFO] üì° Calling embedding service for 1 text(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding text (395 chars):\n",
      "MuseWeaver AI Storytelling Platform\n",
      "MuseWeaver merges constraint‚Äëguided LLM generation with a semantic branching graph, enabling real‚Äëtime co‚Äëauthoring and instant export to game engines or AR web experiences. It solves fragmentation in interactive story workflows and lets small teams ship immersive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:46:02] [INFO] ‚úÖ Got 1 embedding(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding shape: 4096 dimensions\n",
      "First 5 values: [-0.00848388671875, -0.0101318359375, 0.01495361328125, 0.01226806640625, 0.006011962890625]\n"
     ]
    }
   ],
   "source": [
    "from src.clients.embedding import create_embedding_text, get_single_embedding\n",
    "\n",
    "if len(df) > 0:\n",
    "    row = df.iloc[0]\n",
    "    \n",
    "    # Parse hashtags if needed\n",
    "    hashtags = row.get('hashtags', [])\n",
    "    if isinstance(hashtags, str):\n",
    "        try:\n",
    "            hashtags = json.loads(hashtags)\n",
    "        except json.JSONDecodeError:\n",
    "            hashtags = [hashtags]\n",
    "    \n",
    "    # Create embedding text\n",
    "    emb_text = create_embedding_text(\n",
    "        title=str(row.get('title', '')),\n",
    "        description=str(row.get('description', '')),\n",
    "        hashtags=hashtags if isinstance(hashtags, list) else [],\n",
    "    )\n",
    "    print(f'Embedding text ({len(emb_text)} chars):')\n",
    "    print(emb_text[:300])\n",
    "    \n",
    "    # Generate embedding\n",
    "    async def gen_emb():\n",
    "        return await get_single_embedding(emb_text)\n",
    "    \n",
    "    embedding = asyncio.get_event_loop().run_until_complete(gen_emb())\n",
    "    print(f'\\nEmbedding shape: {len(embedding)} dimensions')\n",
    "    print(f'First 5 values: {embedding[:5]}')\n",
    "else:\n",
    "    print('No preview data to test embedding with')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 7. Full Save Test\n",
    "\n",
    "Save a generated project to the database with `source=\"nemo_data_designer\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "save-project",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving project: MuseWeaver AI Storytelling Platform\n",
      "  source: nemo_data_designer\n",
      "  source_comment_id: None\n",
      "  scores: idea=8, complexity=7\n",
      "  hashtags: [\"['ai', 'interactive-storytelling', 'collaborative-narrative', 'game-dev']\"]\n",
      "\n",
      "Saved! Project ID: 453\n",
      "\n",
      "Read back from DB:\n",
      "  id: 453\n",
      "  title: MuseWeaver AI Storytelling Platform\n",
      "  source: nemo_data_designer\n",
      "  source_comment_id: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218/965145048.py:37: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from src.models import WaywoProject\n",
    "from src.db.projects import save_project, get_project\n",
    "\n",
    "def extract_judge_score(quality: dict, score_name: str, default: int = 5) -> int:\n",
    "    \"\"\"Extract an integer score from the NDD judge output.\n",
    "    \n",
    "    Judge columns return: {score_name: {\"score\": N, \"reasoning\": \"...\"}}\n",
    "    \"\"\"\n",
    "    val = quality.get(score_name, default)\n",
    "    if isinstance(val, dict):\n",
    "        val = val.get(\"score\", default)\n",
    "    return max(1, min(10, int(val)))\n",
    "\n",
    "if len(df) > 0:\n",
    "    row = df.iloc[0]\n",
    "    \n",
    "    # Parse fields from the generated row\n",
    "    hashtags = row.get('hashtags', [])\n",
    "    if isinstance(hashtags, str):\n",
    "        try:\n",
    "            hashtags = json.loads(hashtags)\n",
    "        except json.JSONDecodeError:\n",
    "            hashtags = [hashtags]\n",
    "    \n",
    "    # Parse scores from idea_quality\n",
    "    quality = row.get('idea_quality', {})\n",
    "    if isinstance(quality, str):\n",
    "        try:\n",
    "            quality = json.loads(quality)\n",
    "        except json.JSONDecodeError:\n",
    "            quality = {}\n",
    "    \n",
    "    idea_score = extract_judge_score(quality, 'idea_score')\n",
    "    complexity_score = extract_judge_score(quality, 'complexity_score')\n",
    "    \n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    project = WaywoProject(\n",
    "        id=0,  # will be auto-assigned\n",
    "        source_comment_id=None,\n",
    "        source='nemo_data_designer',\n",
    "        is_valid_project=True,\n",
    "        title=str(row.get('title', 'Untitled')),\n",
    "        short_description=str(row.get('short_description', '')),\n",
    "        description=str(row.get('description', '')),\n",
    "        hashtags=hashtags if isinstance(hashtags, list) else [],\n",
    "        project_urls=[],\n",
    "        url_summaries={},\n",
    "        primary_url=None,\n",
    "        url_contents={},\n",
    "        idea_score=idea_score,\n",
    "        complexity_score=complexity_score,\n",
    "        workflow_logs=['Generated by NeMo DataDesigner'],\n",
    "        created_at=now,\n",
    "        processed_at=now,\n",
    "    )\n",
    "    \n",
    "    print(f'Saving project: {project.title}')\n",
    "    print(f'  source: {project.source}')\n",
    "    print(f'  source_comment_id: {project.source_comment_id}')\n",
    "    print(f'  scores: idea={project.idea_score}, complexity={project.complexity_score}')\n",
    "    print(f'  hashtags: {project.hashtags}')\n",
    "    \n",
    "    # Save with embedding\n",
    "    project_id = save_project(project, embedding=embedding if 'embedding' in dir() else None)\n",
    "    print(f'\\nSaved! Project ID: {project_id}')\n",
    "    \n",
    "    # Verify it can be read back\n",
    "    saved = get_project(project_id)\n",
    "    print(f'\\nRead back from DB:')\n",
    "    print(f'  id: {saved.id}')\n",
    "    print(f'  title: {saved.title}')\n",
    "    print(f'  source: {saved.source}')\n",
    "    print(f'  source_comment_id: {saved.source_comment_id}')\n",
    "else:\n",
    "    print('No preview data to save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "verify-api",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects with source=nemo_data_designer: 1\n",
      "  [453] MuseWeaver AI Storytelling Platform (idea=8, complexity=7)\n"
     ]
    }
   ],
   "source": [
    "# Verify it appears in the API with source filter\n",
    "from src.db.projects import get_all_projects\n",
    "\n",
    "ndd_projects = get_all_projects(source='nemo_data_designer')\n",
    "print(f'Projects with source=nemo_data_designer: {len(ndd_projects)}')\n",
    "for p in ndd_projects:\n",
    "    print(f'  [{p.id}] {p.title} (idea={p.idea_score}, complexity={p.complexity_score})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-note",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: delete the test project if you don't want to keep it\n",
    "# from src.db.projects import delete_project\n",
    "# if 'project_id' in dir():\n",
    "#     delete_project(project_id)\n",
    "#     print(f'Deleted test project {project_id}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
